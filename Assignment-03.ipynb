{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.random((30,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random_data[:,0]\n",
    "y = random_data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(X):\n",
    "    return 7.3*X + 9 + random.randint(-2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [func(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2a395a0b7b8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6541092842544374"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a396f4bfd0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGU9JREFUeJzt3Xtw1eWdx/H3VwgYetm4BVuJQmRbU6w4pRtbXXS9G8cBzDAdL1271MtS3anbutMsUtvVdruDY7Taup2xeEN3dvGKgcpWtIKLVaGNjYha0tqqlICCU+JWPWhMnv3jdw4k55JzcnLO7/Kcz2vGkTw5cr4/Ez48+f6e5/mZcw4REUm+A6IuQEREKkOBLiLiCQW6iIgnFOgiIp5QoIuIeEKBLiLiCQW6iIgnFOgiIp5QoIuIeGJ8mG82efJk19TUFOZbiogk3rPPPvumc25KsdeFGuhNTU10dXWF+ZYiIolnZq+V8jq1XEREPKFAFxHxhAJdRMQTCnQREU8o0EVEPKFAFxHxRKjLFkVEytXZ3UvH2h529KWY2lBPe2szbbMboy4rVhToIhJ7nd29LFm5hVT/AAC9fSmWrNwCoFAfQi0XEYm9jrU9+8I8I9U/QMfanogqiicFuojE3o6+1KjGa5UCXURib2pD/ajGa5UCXURir721mfq6ccPG6uvG0d7aHFFF8aSboiISe5kbn1rlMjIFuogkQtvsRgV4EWq5iIh4QoEuIuIJBbqIiCcU6CIinlCgi4h4QoEuIuIJBbqIiCcU6CIinlCgi4h4QoEuIuIJBbqIiCcU6CIinlCgi4hUwwcfhP6WCnQRkUr68pfBDOrq4NOfDvWtdXyuiEglzJwJW7cOH7v//lBLKDpDN7M7zGyXmb2QNX65mfWY2Ytmdl31ShQRiTGz4J+hYX7SSeAczJoVaimltFyWA2cOHTCzk4GzgaOdc58Brq98aSIiMZYJ8qGuuCII8vXrIympaMvFObfBzJqyhi8DrnXOvZd+za7KlyYiEjODgzBuXO74rbfCJZeEX0+Wcm+KHgGcYGabzOx/zeyYShYlIhIr774bzMazw3zdumBGHoMwh/Jvio4HDgKOBY4B7jOzGc45l/1CM1sELAKYNm1auXWKiIRv506YOjV3fOtWaG4Ov54iyp2hbwdWusAvgUFgcr4XOueWOedanHMtU6ZMKbdOEZHwPPdcMCPPDvM33wxm5DEMcyg/0DuBUwDM7AhgAvBmpYoSEYnEqlVBkM+ePXz8vfeCIP/Yx6Kpq0SlLFtcATwDNJvZdjO7GLgDmJFeyngPsDBfu0VEJBFuuCEI8ra24eODg0GQT5gQTV2jVMoql/MLfOqCCtciIhKuCy+E5ctzxxM6P9VOURGpPbNmwQsvDB8bPx76+6Opp0J0louI1I7MZqChYX788cGMPOFhDgp0EakF+XZ1Xn55EORPPhlNTVWglouI+KnQrs5bboGvfjX8ekKgQBcRv6RSMGlS7vijj8Lpp4dfT4gU6CLih9dfh0MOyR3/zW9CP5c8Kuqhi0iyPf980B/PDvNdu4IeeY2EOWiGLiJJtWYNzJ2bO753L0ycGH49Q3R299KxtocdfSmmNtTT3tpM2+zGqr+vZugikiw33RTMyLPDPLOrMwZhvmTlFnr7Ujigty/FkpVb6Ozurfp7a4YuUiVRzdK89dnPwubNueMx29XZsbaHVP/AsLFU/wAda3uq/vVXoItUQWaWlvmDnZmlAQr10cpeP54RsyDP2NGXGtV4JanlIlIFI83SpET5NgNBEOQxDXOAqQ31oxqvJAW6SBVEOUtLvHxB/oUvxD7IM9pbm6mvG76hqb5uHO2t1T9DXYEuUgVRztISaXBw5Icub9wYTV1laJvdyNIFs2hsqMeAxoZ6li6YFUqrTT10kSpob20e1kOH8GZpifLOO/DhD+eOL18OCxeGXk6ltM1ujOReiQJdpAoyf5i1yqWAbdtg+vTc8aefhuOOC78eTyjQRUYwlqWHUc3SYu2pp4LjarNt2waHHRZ+PZ5RD12kgCg3iHjnzjuD/nh2mL/9dtAjV5hXhAJdpAAtPRybzu5efjHzuCDIL7po+Cczuzo/9KFoivOUWi4iBWjpYfncAQfQlmeJYeevt6sNVUUKdJECpjbU05snvLX0cATpZYfZ24GaFj8MQGMI299rmVouIgVEuUEkcQrs6mxa/PC+MAf9dFNtmqGLFBDl0sPEHOxV4JyVOUsf1083EVCgi4wgzKWHmRDv7UthQKYDHbuDvZyDA/L8cN/aCo88AkB71uFkoJ9uwqCWi0gMDF0iCfvDPCMWq2vefjuYkWeH+fe/H4R8Oswh2u3vtUwzdPFSYloWafmWSGaLrP/8+9/DJz+ZO75mDZx1VsH/TBurwqdAF+8k8SzyUsI69P7zY4/BGWfkjvf0wBFHhFuLlEQtF/FOEjcEFQvrUPvP118ftFayw/ytt4LWisI8tjRDF+/EcUNQsRZQvtMZMzdGG8NqGbW1wapVueMDA/lvgkqOzu5evvvTF9nzbj8ADfV1XDP/M6H9ZKhAF+/EbUNQKS2gSE9nrK+HvXtzxxPwMIk46ezupf2BzfQP7P//1pfqp/3+4DmoOg9dJEspNzvjdhZ5qQ8NDv0mYsKe1Rl3HWt7hoV5Rv+gC+UB0aBAlwQp9WZn3M4ij10LSEFeFSN9PcP6WhcNdDO7A5gL7HLOHZX1uW8CHcAU59yb1SlRJFDqTBfitWQuNi2gAkE+89s/C9aIh1uNdwp9nTOfC0MpdzqWA2dmD5rZYcDpwLYK1ySSV5xmup3dvcz+3qM0XbmGpivX8NnvPlrwnPRIz4RxLu85K09NP3rfOStxXwGUFO2tzdSNy/1Ls+4AC63dV3SG7pzbYGZNeT51I/AvQJ7b4iKVF5eZ7mhvfkXSAirwrM4b53yJHx7/pZxxHZo1dpmvZ+JWuZjZfKDXObfZCvXj9r92EbAIYNq0aeW8nQgQn5ud5dz8Cq0F9MorMGNG7viqVTB/Pg9cuw5i8Jeir6Ju9Y16camZTQKuAv61lNc755Y551qccy1TpkwZ7duJ7BOX80HicPMrx7p1QVslO8xfeilou8yfD+hIYN+VM0P/K+BwIDM7PxT4tZl93jn3eiWLE8kW9QwI4nHza58f/hC+8Y3c8T17oKEhZzhuK4CkskYd6M65LcDBmY/N7FWgRatcpFa0tzbn9NAh3JtfnHsu3Hdf7vgHH8C4cbnjQ8ThL0WpjlKWLa4ATgImm9l24Grn3O3VLkwkriK9+dXQEJypkk1ryAUwF+I3QktLi+vq6grt/US8oc1ANc3MnnXOtRR7nXaKisSZglxGQYEuEkcKcimDzsQUiYsCuzo54YTgcwpzKUKBLhK1P/85/7M6v/KVIMQ3bIikLEkeBbpIVF56KQjyj350+Pj11wdBfued0dQliaUeukjY7rkHzj8/d3zDhqC9IlImBbpIWL75TbjhhtzxnTvhE58Ivx7xjgJdpNomTYJUnqMC3n8f6urCr0e8pUAXqRYtPZSQKdBFKk1BLhFRoItUioJcIqZAFxkrBbnEhAJdpBzO5W4EGvo5kQhoY5HIaLzzTv5dnfPmaXu+RE6BLlKKl18Ogjz7wcs33RSE+OrV0dQlMoRaLiIjWbcOTj01d/zJJ+H448OvR2QEmqGL5PODHwQz8uww37EjmJErzCWGNEMXGWrBAnjoodzx/n4Yrz8uEm/6DhUBmDgx2IqfTTc5JUEU6FLbtIZcPKJAl9qkIBcPKdCltijIxWNa5SL+K/SszkMP1WYg8Ypm6DImnd29dKztYUdfiqkN9bS3NtM2uzHqsgJ790J9fe74xRfDbbeFX49IlSnQpWyd3b0sWbmFVP8AAL19KZas3AIQbahv3w6HHZY7ftttQZiLeEotFylbx9qefWGekeofoGNtTzQFbdgQtFWyw3zTpqCtojAXz2mGLmXb0ZfnsWojjFfNj34EX/967vjrr8PHPx5uLUXEukUliadAl4KKhc/Uhnp684T31IY8fetqOPdcuO++3PGYPqszti0q8YZaLpJXJnx6+1I49odPZ3fvvte0tzZTXzdu2H9XXzeO9tbm6haXWbGSHeaZFSsxDHOIYYtKvKNAl7xKCZ+22Y0sXTCLxoZ6DGhsqGfpglnVm23mW3oIiVl6GJsWlXhLLRfJq9TwaZvdWP12gSebgSJvUYn3is7QzewOM9tlZi8MGesws61m9ryZPWRmDdUtU8JWKGRCDZ+Ez8izRdaikppRSstlOXBm1thjwFHOuaOB3wJLKlyXRCzS8PEsyDNCb1FJzSnacnHObTCzpqyxR4d8uBH4YmXLkqhlQia0JXaFdnWeeCI88UR13jMCobSopGZVood+EXBvBX4fiZlQwmfbNpg+PXf83/8dvvWt6r63iGfGFOhmdhXwAfBfI7xmEbAIYNq0aWN5u1jRBpExeuwxOOOM/OOnnRZ+PSIeKHvZopktBOYCf+dc4camc26Zc67FOdcyZcqUct8uVkpZoy0FfP/7QX88O8xfey3ojyvMRcpW1gzdzM4EFgMnOuferWxJ8TfSGm3N0gs48cTgrJVse/cGj38TkTErGuhmtgI4CZhsZtuBqwlWtUwEHrNgNcJG59ylVawzVrRBZBQ8WUMukgSlrHI5P8/w7VWoJTG0QaQECnKR0Gnrfxm0QWQEnq4hF0kCbf0vQ+hrtJNAM3KRyCnQy6QNImkKcpHYUMtFRu/99/O3Vo49Vq0VkQgp0KV0b7wRhHj2MsObbw5C/JlnoqlLRAC1XKQUmzYFs+9sGzbACSeEX4+I5KUZuhR2++3BjDwrzI+7bDlzlj5O54dnRFSYiOSjGbrkuvBCWL48Z/joK1fxfy69XFPPwxSJHc3QZb/GxmBGnh3mzjFn6eP7wzxNz8MUiRfN0KWkpYc67kAk/jRDr2Wj2NUZi0fSiciIFOi1qIzt+TruQCT+1HKpJflC/MADIVW8baLjDkTiT4Huu/5+mDAhd3zBAnjwwVH9VjruQCTe1HLx1e7dwYw8O8xvvDFoq4wyzEUk/jRD901XFxxzTO74unVw8snh1yMiodEM3Rd33RXMyLPDPPOsToW5iPc0Q0+6Sy+Fn/wkdzyVCm54ikjNUKAn1YwZ8MorueODg4U3ComI1xToSaMHSohIAQr0LJ3dvfFca60gF5EiFOhDdHb3smTlFlL9AwD0xuFEQQW5iJRIq1yG6Fjbsy/MMyI7UTDf9nwzPeJNRApSoA8R+YmCAwP5g3zevCDEBwfDqUNEEkmBPkRkJwr29QUhPj6rA3bddUGQr15d3fcXES8o0IcI/UTBrVuDID/ooGHDiy67mcMXP8ycgb+ms7u3Ou8tIt7x+qboaFeshHai4Jo1MHduzvAjj/yKK37xZrxuyopIYpgL8QZbS0uL6+rqCuW9slesQDDbXrpgVnThePfdsHBh7nh6V+eca9fRm6df39hQz1NXnhJCgSISR2b2rHOupdjrvG25xGrFyuLFQWslO8wHB4MeeXqLfuQ3ZUUk0bxtucQiHE85Bdavzxk+fPHDQTvnuR3DflqY2lCfd4aux7yJSCm8DfRIw3HChODBEllmfvtnI/bH21ub87aJ9Jg3ESmFty2XSJ6BmVlDPjTMp08H55iz9PGiLaC22Y0sXTCLxoZ6jKB3HmnPX0QSpegM3czuAOYCu5xzR6XH/hK4F2gCXgXOcc7tqV6ZoxfqMzDzbc8/7zxYsWLfh6W2gPSYNxEpVyktl+XAfwB3Dxm7EnjcOXetmV2Z/nhx5csbm6qG4+AgjBuXO37dddDenjOs/njpYntAmkjMFW25OOc2AH/KGj4buCv967uAtgrXFV+pVDAjzw7z1auDFSt5whwiagElUGa5aW9fCsf+ew3aYCVSXLk99I8753YCpP99cKEXmtkiM+sys67du3eX+XYxkHno8qRJw8c3bw6CfN68Ef9z9cdLE6vlpiIJU/VVLs65ZcAyCDYWVfv9Km7rVpg5M3d8zx5oaBjVb6X+eHGxWG4qklDlztDfMLNDANL/3lW5kmJi/fpgRp4d5u+9F8zIRxnmUprIDkgT8UC5gb4ayGx7XAisqkw5MfD000GQn5K11T6zq3PChGjqqhG61yBSvqKBbmYrgGeAZjPbbmYXA9cCp5vZ74DT0x8n20MPBUE+Z87w8cwDJfTg5VDoXoNI+bw9nKtkN90EV1wxfOyaa+DqqyMpR0QkW6mHc3m79b+ouXODY2yHuv12uOiiaOoRERmj2gv0piZ47bXhYw8+CAsWRFKOiEil1EagOwcH5LldsHYtnHFG+PWIiFSBt4dzAcFDl+fNyw3zzGYghbmIeMTPGfrevdDWFszAh/rjH+HQQ6OpSUSkyvwK9L4+OOmkYAaeMXcuPPAATJwYWVk+6Ozu5bs/fZE97wZHAzfU13HN/M9oOaFIjPgR6Dt3wuzZ8MYb+8cuuQRuuSX/iYgyKp3dvbQ/sJn+gf1LXPtS/bTfH/zFqVAXiYdk99B/97tgw8/UqfvD/DvfCXZ13nqrwrxCOtb2DAvzjP5Bp0OzRGIkmTP0ri445pjhYzffDF/7WjT1eG6kg7F0aJZIfCQv0A8+ODjKNuPee+Gcc6KrpwYUejhH5nMiEg/Ja7lkwvznPw+WHirMq669tZm6cbln2dQdYDo0SyRGkjdDD/HsGQlkbnpqlYtIvCUv0CUSejiHSPwlr+UiIiJ5KdBFRDyhQBcR8YQCXUTEEwp0ERFPaJWLBzq7e+lY28OOvhRTG+ppb23WihSRGqRAT7jO7l6WrNxCqn8AgN6+FEtWbgF0aJZIrVHLJeE61vbsC/OMVP+ADs0SqUEK9IQrdDiWDs0SqT0K9IQrdDiWDs0SqT0K9IRrb22mvm74ue/1deN0aJZIDdJN0YTL3PjUKhcRUaB7QAdniQio5SIi4g0FuoiIJxToIiKeUKCLiHhCgS4i4okxBbqZXWFmL5rZC2a2wswOrFRhIiIyOmUvWzSzRuCfgCOdcykzuw84D1heodpy6FRBEZHCxroOfTxQb2b9wCRgx9hLyk+nCoqIjKzslotzrhe4HtgG7ATecs49WqnCsulUQRGRkZUd6GZ2EHA2cDgwFfiQmV2Q53WLzKzLzLp2795ddqE6VVBEZGRjuSl6GvCKc263c64fWAn8TfaLnHPLnHMtzrmWKVOmlP1mOlVQRGRkYwn0bcCxZjbJzAw4FfhNZcrKpVMFRURGVvZNUefcJjN7APg18AHQDSyrVGHZdKqgiMjIzDkX2pu1tLS4rq6u0N5PRMQHZvasc66l2Ou0U1RExBMKdBERTyjQRUQ8UbNPLNIxAiLim5oMdB0jICI+qsmWi44REBEf1WSg6xgBEfFRTQa6jhEQER/VZKDrGAER8VFN3hTVMQIi4qOaDHQIQl0BLiI+qcmWi4iIjxToIiKeUKCLiHhCgS4i4gkFuoiIJ0J9wIWZ7QZeq8BvNRl4swK/T1LU2vVC7V2zrtd/Y7nm6c65og9lDjXQK8XMukp5eocvau16ofauWdfrvzCuWS0XERFPKNBFRDyR1EBfFnUBIau164Xau2Zdr/+qfs2J7KGLiEiupM7QRUQkS6wD3czONLMeM3vZzK7M8/mJZnZv+vObzKwp/Corp4Tr/Wcze8nMnjezx81sehR1Vkqx6x3yui+amTOzxK+KKOWazeyc9Nf5RTP777BrrKQSvqenmdl6M+tOf1+fFUWdlWJmd5jZLjN7ocDnzcx+lP7/8byZfa6iBTjnYvkPMA74PTADmABsBo7Mes0/Arekf30ecG/UdVf5ek8GJqV/fZnv15t+3UeADcBGoCXqukP4Gn8K6AYOSn98cNR1V/l6lwGXpX99JPBq1HWP8Zr/Fvgc8EKBz58F/Aww4FhgUyXfP84z9M8DLzvn/uCcex+4Bzg76zVnA3elf/0AcKqZWYg1VlLR63XOrXfOvZv+cCNwaMg1VlIpX1+AfwOuA/aGWVyVlHLN/wD82Dm3B8A5tyvkGiuplOt1wEfTv/4LYEeI9VWcc24D8KcRXnI2cLcLbAQazOyQSr1/nAO9EfjjkI+3p8fyvsY59wHwFvCxUKqrvFKud6iLCf6mT6qi12tms4HDnHMPh1lYFZXyNT4COMLMnjKzjWZ2ZmjVVV4p13sNcIGZbQf+B7g8nNIiM9o/56MS5wdc5JtpZy/JKeU1SVHytZjZBUALcGJVK6quEa/XzA4AbgS+ElZBISjlazyeoO1yEsFPYE+a2VHOub4q11YNpVzv+cBy59wNZnYc8J/p6x2sfnmRqGpmxXmGvh04bMjHh5L749i+15jZeIIf2Ub6cSfOSrlezOw04CpgvnPuvZBqq4Zi1/sR4CjgCTN7laDfuDrhN0ZL/Z5e5Zzrd869AvQQBHwSlXK9FwP3ATjnngEOJDjzxFcl/TkvV5wD/VfAp8zscDObQHDTc3XWa1YDC9O//iKwzqXvPCRQ0etNtyB+QhDmSe6tQpHrdc695Zyb7Jxrcs41EdwzmO+c64qm3Ioo5Xu6k+DmN2Y2maAF84dQq6ycUq53G3AqgJnNJAj03aFWGa7VwN+nV7scC7zlnNtZsd896rvCRe4YnwX8luBO+VXpse8R/MGG4It/P/Ay8EtgRtQ1V/l6fw68ATyX/md11DVX83qzXvsECV/lUuLX2IAfAC8BW4Dzoq65ytd7JPAUwQqY54Azoq55jNe7AtgJ9BPMxi8GLgUuHfL1/XH6/8eWSn9Pa6eoiIgn4txyERGRUVCgi4h4QoEuIuIJBbqIiCcU6CIinlCgi4h4QoEuIuIJBbqIiCf+Hy2/Pxm00GneAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X,y_pred,color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "def model(X,y):\n",
    "    return [(Xi,yi) for Xi,yi in zip(X,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1,x2):\n",
    "    return cosine(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, k=5):\n",
    "    most_similar = sorted(model(X,y), key = lambda xi: distance(xi[0],x))[:k]\n",
    "    y_pred = sum(p[1] for p in most_similar)/k\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.783617069169434"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "def entropy(elements):\n",
    "    count = Counter(elements)\n",
    "    probs = [count[c]/len(elements) for c in set(elements)]\n",
    "    return sum(-p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bought', 'family_number', 'income'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pd.DataFrame(mock_data).columns.tolist())-{'gender'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_spliter(trainingdata: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(trainingdata.columns.tolist())-{target}\n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(trainingdata[f]) \n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = trainingdata[trainingdata[f] == v][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_1)\n",
    "            sub_spliter_2 = trainingdata[trainingdata[f] != v][target].tolist()\n",
    "            ic(sub_spliter_2)\n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_2)\n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v < min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f,v)\n",
    "                \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: 0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: 0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: 0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: 0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '-10')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_min_spliter(pd.DataFrame(mock_data), 'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [random.randint(0,100) for _ in range(100)]\n",
    "Y = [random.randint(0,100) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2a39730b278>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[x,y] for x, y in zip(X,Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 35]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=5,max_iter = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70.96      , 17.6       ],\n",
       "       [50.0952381 , 69.33333333],\n",
       "       [22.05882353, 21.82352941],\n",
       "       [15.82352941, 67.29411765],\n",
       "       [86.3       , 71.95      ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, location in zip(cluster.labels_, train_data):\n",
    "    centers[label].append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['red','green','grey','black','yellow']\n",
    "\n",
    "for i,c in enumerate(centers):\n",
    "    #print(i,c)\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location,c = color[i])\n",
    "\n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center,s = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Model is built based on historial data and can be used to explain a certain pattern and make prediction when new data point comes in. Since all models are built on previous data or samples, it can never provide the information for furture situation once things change. However, some models can be useful if there is a pattern for a certain group of data and can be captured by model, then we could use these modelse to make prediction of other data points which are in the same distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: underfitting is a model can not capture a certain trend of the data, it shows low variance but high bias. Underfitting is oftern a reason of very simple model.\n",
    "\n",
    "Overfitting happens when a model fit the data so well that it even capture the noise of those training data samples, it shows low bias but high variance.And it's usually caused by a much complicated model or the training data is not well samples so that can not represent the whole distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Ans >:\n",
    "> + precision: is the fraction of relevant instances among the retrieved instances. It's the fraction of true positives counts with all predicted positives counts\n",
    "+ recal : is the fraction of the total amount of relevant instances that were actually retrieved. It's the fraction true posiitivese counts with all real positives counts\n",
    "+ AUC: it's the area under ROC curve. It measures the performance of a certain model at various threshold settings, and tells how much a model is capable of distinguishing between classes. The higher the AUC, the better the model predict. \n",
    "+ F1: it's a harmonic mean of precision and recall , it's a measure of test's accuracy.\n",
    "+ F2: is the situation when βin F-score generation formula is set to 2.The intuition behind the F2 score is that it weights recall higher than precision. This makes the F2 score more suitable in certain applications where it’s more important to classify correctly as many positive samples as possible, rather than maximizing the number of correct classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate(dataset,gender, income, family_number):\n",
    "    params = {'gender':gender, 'income':income, 'family_number':family_number}\n",
    "    min_spliter_1= find_min_spliter(dataset, 'bought')\n",
    "    f_1,v_1 = min_spliter_1[0],min_spliter_1[1]\n",
    "    \n",
    "    if params[f_1] == v_1:\n",
    "        return dataset[dataset[f_1] == v_1]['bought'].unique().tolist()[0]\n",
    "    else:\n",
    "        return predicate(dataset[dataset[f_1] != v_1],gender, income, family_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: 0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: 0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: 0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: 0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| entropy_2: 0.0\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| entropy_1: 0.0\n",
      "ic| sub_spliter_2: [1, 1, 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_2: 0.0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| entropy_1: 0.0\n",
      "ic| sub_spliter_2: [1, 0, 0, 0]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 0.5623351446188083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 1)\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate(dataset,'F',10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "from sklearn.datasets import load_boston\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "x,y = dataset['data'],dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,y_hat):\n",
    "    return sum(abs(y_i-y_hat_i) for y_i,y_hat_i in zip(list(y),list(y_hat))) / len(list(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(rm,k,b):\n",
    "    return k*rm+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_k(x,y,y_hat):\n",
    "    n = len(y)\n",
    "    grad = 0\n",
    "    for x_i,y_i,y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if k*x_i+b < y_i:       \n",
    "            grad += -x_i\n",
    "        elif k*x_i > y_i:\n",
    "            grad += x_i\n",
    "        else:\n",
    "            grad += 0\n",
    "    return grad/n\n",
    "\n",
    "def grad_b(x,y,y_hat):\n",
    "    n = len(y)\n",
    "    grad = 0\n",
    "    for x_i,y_i,y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if k*x_i+b < y_i:       \n",
    "            grad += -1\n",
    "        elif k*x_i > y_i:\n",
    "            grad += 1\n",
    "        else:\n",
    "            grad += 0\n",
    "    return grad/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 300.1555758490754, parameters k is 55.75142995280319 and b is -27.68897165223487\n",
      "Iteration 1, the loss is 299.7506095552487, parameters k is 55.688583608929676 and b is -27.69897165223487\n",
      "Iteration 2, the loss is 299.3456432614217, parameters k is 55.62573726505616 and b is -27.708971652234872\n",
      "Iteration 3, the loss is 298.940676967595, parameters k is 55.56289092118264 and b is -27.718971652234874\n",
      "Iteration 4, the loss is 298.53571067376794, parameters k is 55.500044577309126 and b is -27.728971652234875\n",
      "Iteration 5, the loss is 298.1307443799413, parameters k is 55.43719823343561 and b is -27.738971652234877\n",
      "Iteration 6, the loss is 297.7257780861143, parameters k is 55.37435188956209 and b is -27.74897165223488\n",
      "Iteration 7, the loss is 297.3208117922876, parameters k is 55.311505545688576 and b is -27.75897165223488\n",
      "Iteration 8, the loss is 296.91584549846056, parameters k is 55.24865920181506 and b is -27.76897165223488\n",
      "Iteration 9, the loss is 296.51087920463385, parameters k is 55.18581285794154 and b is -27.778971652234883\n",
      "Iteration 10, the loss is 296.1059129108068, parameters k is 55.122966514068025 and b is -27.788971652234885\n",
      "Iteration 11, the loss is 295.70094661698016, parameters k is 55.06012017019451 and b is -27.798971652234886\n",
      "Iteration 12, the loss is 295.29598032315334, parameters k is 54.99727382632099 and b is -27.808971652234888\n",
      "Iteration 13, the loss is 294.89101402932636, parameters k is 54.934427482447475 and b is -27.81897165223489\n",
      "Iteration 14, the loss is 294.48604773549977, parameters k is 54.87158113857396 and b is -27.82897165223489\n",
      "Iteration 15, the loss is 294.081081441673, parameters k is 54.80873479470044 and b is -27.838971652234893\n",
      "Iteration 16, the loss is 293.6761151478459, parameters k is 54.745888450826925 and b is -27.848971652234894\n",
      "Iteration 17, the loss is 293.27114885401903, parameters k is 54.68304210695341 and b is -27.858971652234896\n",
      "Iteration 18, the loss is 292.86618256019256, parameters k is 54.62019576307989 and b is -27.868971652234897\n",
      "Iteration 19, the loss is 292.4612162663651, parameters k is 54.557349419206375 and b is -27.8789716522349\n",
      "Iteration 20, the loss is 292.0562499725386, parameters k is 54.49450307533286 and b is -27.8889716522349\n",
      "Iteration 21, the loss is 291.65128367871165, parameters k is 54.43165673145934 and b is -27.898971652234902\n",
      "Iteration 22, the loss is 291.2463173848846, parameters k is 54.368810387585825 and b is -27.908971652234904\n",
      "Iteration 23, the loss is 290.841351091058, parameters k is 54.30596404371231 and b is -27.918971652234905\n",
      "Iteration 24, the loss is 290.436384797231, parameters k is 54.24311769983879 and b is -27.928971652234907\n",
      "Iteration 25, the loss is 290.0314185034044, parameters k is 54.180271355965274 and b is -27.93897165223491\n",
      "Iteration 26, the loss is 289.62645220957756, parameters k is 54.11742501209176 and b is -27.94897165223491\n",
      "Iteration 27, the loss is 289.2214859157508, parameters k is 54.05457866821824 and b is -27.95897165223491\n",
      "Iteration 28, the loss is 288.8165196219237, parameters k is 53.991732324344724 and b is -27.968971652234913\n",
      "Iteration 29, the loss is 288.411553328097, parameters k is 53.92888598047121 and b is -27.978971652234915\n",
      "Iteration 30, the loss is 288.0065870342703, parameters k is 53.86603963659769 and b is -27.988971652234916\n",
      "Iteration 31, the loss is 287.6016207404434, parameters k is 53.803193292724174 and b is -27.998971652234918\n",
      "Iteration 32, the loss is 287.1966544466165, parameters k is 53.74034694885066 and b is -28.00897165223492\n",
      "Iteration 33, the loss is 286.7916881527896, parameters k is 53.67750060497714 and b is -28.01897165223492\n",
      "Iteration 34, the loss is 286.3867218589626, parameters k is 53.614654261103624 and b is -28.028971652234922\n",
      "Iteration 35, the loss is 285.9817555651359, parameters k is 53.55180791723011 and b is -28.038971652234924\n",
      "Iteration 36, the loss is 285.5767892713089, parameters k is 53.48896157335659 and b is -28.048971652234926\n",
      "Iteration 37, the loss is 285.17182297748246, parameters k is 53.426115229483074 and b is -28.058971652234927\n",
      "Iteration 38, the loss is 284.7668566836557, parameters k is 53.36326888560956 and b is -28.06897165223493\n",
      "Iteration 39, the loss is 284.3618903898284, parameters k is 53.30042254173604 and b is -28.07897165223493\n",
      "Iteration 40, the loss is 283.9569240960016, parameters k is 53.23757619786252 and b is -28.08897165223493\n",
      "Iteration 41, the loss is 283.5519578021751, parameters k is 53.17472985398901 and b is -28.098971652234933\n",
      "Iteration 42, the loss is 283.14699150834826, parameters k is 53.11188351011549 and b is -28.108971652234935\n",
      "Iteration 43, the loss is 282.74202521452094, parameters k is 53.04903716624197 and b is -28.118971652234936\n",
      "Iteration 44, the loss is 282.3370589206944, parameters k is 52.98619082236846 and b is -28.128971652234938\n",
      "Iteration 45, the loss is 281.93209262686753, parameters k is 52.92334447849494 and b is -28.13897165223494\n",
      "Iteration 46, the loss is 281.5271263330406, parameters k is 52.86049813462142 and b is -28.14897165223494\n",
      "Iteration 47, the loss is 281.1221600392141, parameters k is 52.797651790747906 and b is -28.158971652234943\n",
      "Iteration 48, the loss is 280.7171937453869, parameters k is 52.73480544687439 and b is -28.168971652234944\n",
      "Iteration 49, the loss is 280.3122274515604, parameters k is 52.67195910300087 and b is -28.178971652234946\n",
      "Iteration 50, the loss is 279.9072611577334, parameters k is 52.609112759127356 and b is -28.188971652234947\n",
      "Iteration 51, the loss is 279.5022948639067, parameters k is 52.54626641525384 and b is -28.19897165223495\n",
      "Iteration 52, the loss is 279.0973285700799, parameters k is 52.48342007138032 and b is -28.20897165223495\n",
      "Iteration 53, the loss is 278.6923622762526, parameters k is 52.420573727506806 and b is -28.218971652234952\n",
      "Iteration 54, the loss is 278.28739598242606, parameters k is 52.35772738363329 and b is -28.228971652234954\n",
      "Iteration 55, the loss is 277.8824296885992, parameters k is 52.29488103975977 and b is -28.238971652234955\n",
      "Iteration 56, the loss is 277.47746339477214, parameters k is 52.232034695886256 and b is -28.248971652234957\n",
      "Iteration 57, the loss is 277.0724971009453, parameters k is 52.16918835201274 and b is -28.25897165223496\n",
      "Iteration 58, the loss is 276.6675308071187, parameters k is 52.10634200813922 and b is -28.26897165223496\n",
      "Iteration 59, the loss is 276.2625645132918, parameters k is 52.043495664265706 and b is -28.27897165223496\n",
      "Iteration 60, the loss is 275.8575982194648, parameters k is 51.98064932039219 and b is -28.288971652234963\n",
      "Iteration 61, the loss is 275.4526319256381, parameters k is 51.91780297651867 and b is -28.298971652234965\n",
      "Iteration 62, the loss is 275.04766563181124, parameters k is 51.854956632645155 and b is -28.308971652234966\n",
      "Iteration 63, the loss is 274.6426993379842, parameters k is 51.79211028877164 and b is -28.318971652234968\n",
      "Iteration 64, the loss is 274.2377330441576, parameters k is 51.72926394489812 and b is -28.32897165223497\n",
      "Iteration 65, the loss is 273.83276675033073, parameters k is 51.666417601024605 and b is -28.33897165223497\n",
      "Iteration 66, the loss is 273.427800456504, parameters k is 51.60357125715109 and b is -28.348971652234972\n",
      "Iteration 67, the loss is 273.02283416267716, parameters k is 51.54072491327757 and b is -28.358971652234974\n",
      "Iteration 68, the loss is 272.6178678688503, parameters k is 51.477878569404055 and b is -28.368971652234976\n",
      "Iteration 69, the loss is 272.2129015750232, parameters k is 51.41503222553054 and b is -28.378971652234977\n",
      "Iteration 70, the loss is 271.8079352811969, parameters k is 51.35218588165702 and b is -28.38897165223498\n",
      "Iteration 71, the loss is 271.40296898736966, parameters k is 51.289339537783505 and b is -28.39897165223498\n",
      "Iteration 72, the loss is 270.9980026935433, parameters k is 51.22649319390999 and b is -28.40897165223498\n",
      "Iteration 73, the loss is 270.59303639971625, parameters k is 51.16364685003647 and b is -28.418971652234983\n",
      "Iteration 74, the loss is 270.1880701058892, parameters k is 51.100800506162955 and b is -28.428971652234985\n",
      "Iteration 75, the loss is 269.78310381206217, parameters k is 51.03795416228944 and b is -28.438971652234986\n",
      "Iteration 76, the loss is 269.3781375182354, parameters k is 50.97510781841592 and b is -28.448971652234988\n",
      "Iteration 77, the loss is 268.97317122440876, parameters k is 50.912261474542404 and b is -28.45897165223499\n",
      "Iteration 78, the loss is 268.56820493058194, parameters k is 50.84941513066889 and b is -28.46897165223499\n",
      "Iteration 79, the loss is 268.1632386367552, parameters k is 50.78656878679537 and b is -28.478971652234993\n",
      "Iteration 80, the loss is 267.75827234292836, parameters k is 50.723722442921854 and b is -28.488971652234994\n",
      "Iteration 81, the loss is 267.35330604910115, parameters k is 50.66087609904834 and b is -28.498971652234996\n",
      "Iteration 82, the loss is 266.9483397552744, parameters k is 50.59802975517482 and b is -28.508971652234997\n",
      "Iteration 83, the loss is 266.5433734614477, parameters k is 50.535183411301304 and b is -28.518971652235\n",
      "Iteration 84, the loss is 266.1384071676205, parameters k is 50.47233706742779 and b is -28.528971652235\n",
      "Iteration 85, the loss is 265.73344087379394, parameters k is 50.40949072355427 and b is -28.538971652235002\n",
      "Iteration 86, the loss is 265.32847457996724, parameters k is 50.346644379680754 and b is -28.548971652235004\n",
      "Iteration 87, the loss is 264.9235082861405, parameters k is 50.28379803580724 and b is -28.558971652235005\n",
      "Iteration 88, the loss is 264.51854199231326, parameters k is 50.22095169193372 and b is -28.568971652235007\n",
      "Iteration 89, the loss is 264.1135756984867, parameters k is 50.158105348060204 and b is -28.57897165223501\n",
      "Iteration 90, the loss is 263.7086094046599, parameters k is 50.09525900418669 and b is -28.58897165223501\n",
      "Iteration 91, the loss is 263.303643110833, parameters k is 50.03241266031317 and b is -28.59897165223501\n",
      "Iteration 92, the loss is 262.898676817006, parameters k is 49.96956631643965 and b is -28.608971652235013\n",
      "Iteration 93, the loss is 262.4937105231792, parameters k is 49.90671997256614 and b is -28.618971652235015\n",
      "Iteration 94, the loss is 262.0887442293524, parameters k is 49.84387362869262 and b is -28.628971652235016\n",
      "Iteration 95, the loss is 261.6837779355256, parameters k is 49.7810272848191 and b is -28.638971652235018\n",
      "Iteration 96, the loss is 261.27881164169884, parameters k is 49.71818094094559 and b is -28.64897165223502\n",
      "Iteration 97, the loss is 260.87384534787174, parameters k is 49.65533459707207 and b is -28.65897165223502\n",
      "Iteration 98, the loss is 260.46887905404486, parameters k is 49.59248825319855 and b is -28.668971652235022\n",
      "Iteration 99, the loss is 260.06391276021833, parameters k is 49.529641909325036 and b is -28.678971652235024\n",
      "Iteration 100, the loss is 259.65894646639134, parameters k is 49.46679556545152 and b is -28.688971652235026\n",
      "Iteration 101, the loss is 259.2539801725648, parameters k is 49.403949221578 and b is -28.698971652235027\n",
      "Iteration 102, the loss is 258.8490138787379, parameters k is 49.341102877704486 and b is -28.70897165223503\n",
      "Iteration 103, the loss is 258.4440475849109, parameters k is 49.27825653383097 and b is -28.71897165223503\n",
      "Iteration 104, the loss is 258.03908129108413, parameters k is 49.21541018995745 and b is -28.728971652235032\n",
      "Iteration 105, the loss is 257.63411499725714, parameters k is 49.152563846083936 and b is -28.738971652235033\n",
      "Iteration 106, the loss is 257.22914870343055, parameters k is 49.08971750221042 and b is -28.748971652235035\n",
      "Iteration 107, the loss is 256.82418240960357, parameters k is 49.0268711583369 and b is -28.758971652235036\n",
      "Iteration 108, the loss is 256.41921611577663, parameters k is 48.964024814463386 and b is -28.768971652235038\n",
      "Iteration 109, the loss is 256.01424982194993, parameters k is 48.90117847058987 and b is -28.77897165223504\n",
      "Iteration 110, the loss is 255.60928352812272, parameters k is 48.83833212671635 and b is -28.78897165223504\n",
      "Iteration 111, the loss is 255.20431723429618, parameters k is 48.775485782842836 and b is -28.798971652235043\n",
      "Iteration 112, the loss is 254.79935094046957, parameters k is 48.71263943896932 and b is -28.808971652235044\n",
      "Iteration 113, the loss is 254.39438464664264, parameters k is 48.6497930950958 and b is -28.818971652235046\n",
      "Iteration 114, the loss is 253.98941835281536, parameters k is 48.586946751222285 and b is -28.828971652235047\n",
      "Iteration 115, the loss is 253.58445205898877, parameters k is 48.52410040734877 and b is -28.83897165223505\n",
      "Iteration 116, the loss is 253.179485765162, parameters k is 48.46125406347525 and b is -28.84897165223505\n",
      "Iteration 117, the loss is 252.77451947133517, parameters k is 48.398407719601735 and b is -28.858971652235052\n",
      "Iteration 118, the loss is 252.36955317750812, parameters k is 48.33556137572822 and b is -28.868971652235054\n",
      "Iteration 119, the loss is 251.96458688368148, parameters k is 48.2727150318547 and b is -28.878971652235055\n",
      "Iteration 120, the loss is 251.5596205898544, parameters k is 48.209868687981185 and b is -28.888971652235057\n",
      "Iteration 121, the loss is 251.15465429602776, parameters k is 48.14702234410767 and b is -28.89897165223506\n",
      "Iteration 122, the loss is 250.74968800220103, parameters k is 48.08417600023415 and b is -28.90897165223506\n",
      "Iteration 123, the loss is 250.34472170837392, parameters k is 48.021329656360635 and b is -28.91897165223506\n",
      "Iteration 124, the loss is 249.9397554145473, parameters k is 47.95848331248712 and b is -28.928971652235063\n",
      "Iteration 125, the loss is 249.53478912072038, parameters k is 47.8956369686136 and b is -28.938971652235065\n",
      "Iteration 126, the loss is 249.12982282689356, parameters k is 47.832790624740085 and b is -28.948971652235066\n",
      "Iteration 127, the loss is 248.72485653306663, parameters k is 47.76994428086657 and b is -28.958971652235068\n",
      "Iteration 128, the loss is 248.31989023923992, parameters k is 47.70709793699305 and b is -28.96897165223507\n",
      "Iteration 129, the loss is 247.914923945413, parameters k is 47.644251593119535 and b is -28.97897165223507\n",
      "Iteration 130, the loss is 247.50995765158635, parameters k is 47.58140524924602 and b is -28.988971652235072\n",
      "Iteration 131, the loss is 247.1049913577596, parameters k is 47.5185589053725 and b is -28.998971652235074\n",
      "Iteration 132, the loss is 246.7000250639327, parameters k is 47.455712561498984 and b is -29.008971652235076\n",
      "Iteration 133, the loss is 246.2950587701056, parameters k is 47.39286621762547 and b is -29.018971652235077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 134, the loss is 245.89009247627885, parameters k is 47.33001987375195 and b is -29.02897165223508\n",
      "Iteration 135, the loss is 245.485126182452, parameters k is 47.267173529878434 and b is -29.03897165223508\n",
      "Iteration 136, the loss is 245.0801598886252, parameters k is 47.20432718600492 and b is -29.048971652235082\n",
      "Iteration 137, the loss is 244.67519359479826, parameters k is 47.1414808421314 and b is -29.058971652235083\n",
      "Iteration 138, the loss is 244.27022730097164, parameters k is 47.078634498257884 and b is -29.068971652235085\n",
      "Iteration 139, the loss is 243.8652610071446, parameters k is 47.01578815438437 and b is -29.078971652235087\n",
      "Iteration 140, the loss is 243.4602947133179, parameters k is 46.95294181051085 and b is -29.088971652235088\n",
      "Iteration 141, the loss is 243.05532841949085, parameters k is 46.890095466637334 and b is -29.09897165223509\n",
      "Iteration 142, the loss is 242.650362125664, parameters k is 46.82724912276382 and b is -29.10897165223509\n",
      "Iteration 143, the loss is 242.2453958318375, parameters k is 46.7644027788903 and b is -29.118971652235093\n",
      "Iteration 144, the loss is 241.84042953801014, parameters k is 46.701556435016784 and b is -29.128971652235094\n",
      "Iteration 145, the loss is 241.4354632441836, parameters k is 46.63871009114327 and b is -29.138971652235096\n",
      "Iteration 146, the loss is 241.03049695035693, parameters k is 46.57586374726975 and b is -29.148971652235097\n",
      "Iteration 147, the loss is 240.62553065652997, parameters k is 46.51301740339623 and b is -29.1589716522351\n",
      "Iteration 148, the loss is 240.22056436270293, parameters k is 46.45017105952272 and b is -29.1689716522351\n",
      "Iteration 149, the loss is 239.81559806887591, parameters k is 46.3873247156492 and b is -29.178971652235102\n",
      "Iteration 150, the loss is 239.41063177504935, parameters k is 46.32447837177568 and b is -29.188971652235104\n",
      "Iteration 151, the loss is 239.0056654812226, parameters k is 46.261632027902166 and b is -29.198971652235105\n",
      "Iteration 152, the loss is 238.60069918739583, parameters k is 46.19878568402865 and b is -29.208971652235107\n",
      "Iteration 153, the loss is 238.19573289356896, parameters k is 46.13593934015513 and b is -29.21897165223511\n",
      "Iteration 154, the loss is 237.79076659974208, parameters k is 46.073092996281616 and b is -29.22897165223511\n",
      "Iteration 155, the loss is 237.38580030591527, parameters k is 46.0102466524081 and b is -29.23897165223511\n",
      "Iteration 156, the loss is 236.9808340120883, parameters k is 45.94740030853458 and b is -29.248971652235113\n",
      "Iteration 157, the loss is 236.5758677182616, parameters k is 45.884553964661066 and b is -29.258971652235115\n",
      "Iteration 158, the loss is 236.17090142443476, parameters k is 45.82170762078755 and b is -29.268971652235116\n",
      "Iteration 159, the loss is 235.76593513060794, parameters k is 45.75886127691403 and b is -29.278971652235118\n",
      "Iteration 160, the loss is 235.36096883678115, parameters k is 45.696014933040516 and b is -29.28897165223512\n",
      "Iteration 161, the loss is 234.95600254295454, parameters k is 45.633168589167 and b is -29.29897165223512\n",
      "Iteration 162, the loss is 234.55103624912746, parameters k is 45.57032224529348 and b is -29.308971652235122\n",
      "Iteration 163, the loss is 234.1460699553006, parameters k is 45.507475901419966 and b is -29.318971652235124\n",
      "Iteration 164, the loss is 233.74110366147357, parameters k is 45.44462955754645 and b is -29.328971652235126\n",
      "Iteration 165, the loss is 233.33613736764678, parameters k is 45.38178321367293 and b is -29.338971652235127\n",
      "Iteration 166, the loss is 232.93117107381994, parameters k is 45.318936869799415 and b is -29.34897165223513\n",
      "Iteration 167, the loss is 232.5262047799931, parameters k is 45.2560905259259 and b is -29.35897165223513\n",
      "Iteration 168, the loss is 232.12123848616622, parameters k is 45.19324418205238 and b is -29.368971652235132\n",
      "Iteration 169, the loss is 231.71627219233955, parameters k is 45.130397838178865 and b is -29.378971652235133\n",
      "Iteration 170, the loss is 231.31130589851247, parameters k is 45.06755149430535 and b is -29.388971652235135\n",
      "Iteration 171, the loss is 230.90633960468583, parameters k is 45.00470515043183 and b is -29.398971652235137\n",
      "Iteration 172, the loss is 230.50137331085878, parameters k is 44.941858806558315 and b is -29.408971652235138\n",
      "Iteration 173, the loss is 230.09640701703213, parameters k is 44.8790124626848 and b is -29.41897165223514\n",
      "Iteration 174, the loss is 229.69144072320518, parameters k is 44.81616611881128 and b is -29.42897165223514\n",
      "Iteration 175, the loss is 229.2864744293783, parameters k is 44.753319774937765 and b is -29.438971652235143\n",
      "Iteration 176, the loss is 228.88150813555166, parameters k is 44.69047343106425 and b is -29.448971652235144\n",
      "Iteration 177, the loss is 228.47654184172504, parameters k is 44.62762708719073 and b is -29.458971652235146\n",
      "Iteration 178, the loss is 228.071575547898, parameters k is 44.564780743317215 and b is -29.468971652235147\n",
      "Iteration 179, the loss is 227.66660925407118, parameters k is 44.5019343994437 and b is -29.47897165223515\n",
      "Iteration 180, the loss is 227.26164296024402, parameters k is 44.43908805557018 and b is -29.48897165223515\n",
      "Iteration 181, the loss is 226.8566766664173, parameters k is 44.376241711696665 and b is -29.498971652235152\n",
      "Iteration 182, the loss is 226.45171037259053, parameters k is 44.31339536782315 and b is -29.508971652235154\n",
      "Iteration 183, the loss is 226.0467440787641, parameters k is 44.25054902394963 and b is -29.518971652235155\n",
      "Iteration 184, the loss is 225.64177778493672, parameters k is 44.187702680076114 and b is -29.528971652235157\n",
      "Iteration 185, the loss is 225.23681149111, parameters k is 44.1248563362026 and b is -29.53897165223516\n",
      "Iteration 186, the loss is 224.8318451972833, parameters k is 44.06200999232908 and b is -29.54897165223516\n",
      "Iteration 187, the loss is 224.42687890345636, parameters k is 43.999163648455564 and b is -29.55897165223516\n",
      "Iteration 188, the loss is 224.0219126096296, parameters k is 43.93631730458205 and b is -29.568971652235163\n",
      "Iteration 189, the loss is 223.61694631580272, parameters k is 43.87347096070853 and b is -29.578971652235165\n",
      "Iteration 190, the loss is 223.2119800219758, parameters k is 43.810624616835014 and b is -29.588971652235166\n",
      "Iteration 191, the loss is 222.80701372814903, parameters k is 43.7477782729615 and b is -29.598971652235168\n",
      "Iteration 192, the loss is 222.40204743432218, parameters k is 43.68493192908798 and b is -29.60897165223517\n",
      "Iteration 193, the loss is 221.99708114049542, parameters k is 43.622085585214464 and b is -29.61897165223517\n",
      "Iteration 194, the loss is 221.59211484666847, parameters k is 43.55923924134095 and b is -29.628971652235172\n",
      "Iteration 195, the loss is 221.1871485528416, parameters k is 43.49639289746743 and b is -29.638971652235174\n",
      "Iteration 196, the loss is 220.78218225901486, parameters k is 43.433546553593914 and b is -29.648971652235176\n",
      "Iteration 197, the loss is 220.37721596518801, parameters k is 43.3707002097204 and b is -29.658971652235177\n",
      "Iteration 198, the loss is 219.97224967136108, parameters k is 43.30785386584688 and b is -29.66897165223518\n",
      "Iteration 199, the loss is 219.56728337753444, parameters k is 43.24500752197336 and b is -29.67897165223518\n",
      "Iteration 200, the loss is 219.16231708370745, parameters k is 43.18216117809985 and b is -29.688971652235182\n",
      "Iteration 201, the loss is 218.7573507898805, parameters k is 43.11931483422633 and b is -29.698971652235183\n",
      "Iteration 202, the loss is 218.3523844960539, parameters k is 43.05646849035281 and b is -29.708971652235185\n",
      "Iteration 203, the loss is 217.94741820222703, parameters k is 42.9936221464793 and b is -29.718971652235187\n",
      "Iteration 204, the loss is 217.54245190840032, parameters k is 42.93077580260578 and b is -29.728971652235188\n",
      "Iteration 205, the loss is 217.13748561457314, parameters k is 42.86792945873226 and b is -29.73897165223519\n",
      "Iteration 206, the loss is 216.73251932074638, parameters k is 42.805083114858746 and b is -29.74897165223519\n",
      "Iteration 207, the loss is 216.32755302691953, parameters k is 42.74223677098523 and b is -29.758971652235193\n",
      "Iteration 208, the loss is 215.92258673309274, parameters k is 42.67939042711171 and b is -29.768971652235194\n",
      "Iteration 209, the loss is 215.51762043926587, parameters k is 42.616544083238196 and b is -29.778971652235196\n",
      "Iteration 210, the loss is 215.11265414543902, parameters k is 42.55369773936468 and b is -29.788971652235197\n",
      "Iteration 211, the loss is 214.70768785161212, parameters k is 42.49085139549116 and b is -29.7989716522352\n",
      "Iteration 212, the loss is 214.30272155778553, parameters k is 42.428005051617646 and b is -29.8089716522352\n",
      "Iteration 213, the loss is 213.89775526395866, parameters k is 42.36515870774413 and b is -29.818971652235202\n",
      "Iteration 214, the loss is 213.49278897013147, parameters k is 42.30231236387061 and b is -29.828971652235204\n",
      "Iteration 215, the loss is 213.0878226763047, parameters k is 42.239466019997096 and b is -29.838971652235205\n",
      "Iteration 216, the loss is 212.682856382478, parameters k is 42.17661967612358 and b is -29.848971652235207\n",
      "Iteration 217, the loss is 212.27789008865108, parameters k is 42.11377333225006 and b is -29.85897165223521\n",
      "Iteration 218, the loss is 211.87292379482443, parameters k is 42.050926988376546 and b is -29.86897165223521\n",
      "Iteration 219, the loss is 211.46795750099736, parameters k is 41.98808064450303 and b is -29.87897165223521\n",
      "Iteration 220, the loss is 211.0629912071706, parameters k is 41.92523430062951 and b is -29.888971652235213\n",
      "Iteration 221, the loss is 210.65802491334367, parameters k is 41.862387956755995 and b is -29.898971652235215\n",
      "Iteration 222, the loss is 210.25305861951693, parameters k is 41.79954161288248 and b is -29.908971652235216\n",
      "Iteration 223, the loss is 209.84809232569012, parameters k is 41.73669526900896 and b is -29.918971652235218\n",
      "Iteration 224, the loss is 209.44312603186327, parameters k is 41.673848925135445 and b is -29.92897165223522\n",
      "Iteration 225, the loss is 209.03815973803626, parameters k is 41.61100258126193 and b is -29.93897165223522\n",
      "Iteration 226, the loss is 208.6331934442098, parameters k is 41.54815623738841 and b is -29.948971652235223\n",
      "Iteration 227, the loss is 208.22822715038302, parameters k is 41.485309893514895 and b is -29.958971652235224\n",
      "Iteration 228, the loss is 207.82326085655592, parameters k is 41.42246354964138 and b is -29.968971652235226\n",
      "Iteration 229, the loss is 207.41829456272893, parameters k is 41.35961720576786 and b is -29.978971652235227\n",
      "Iteration 230, the loss is 207.01332826890246, parameters k is 41.296770861894345 and b is -29.98897165223523\n",
      "Iteration 231, the loss is 206.60836197507544, parameters k is 41.23392451802083 and b is -29.99897165223523\n",
      "Iteration 232, the loss is 206.20339568124857, parameters k is 41.17107817414731 and b is -30.008971652235232\n",
      "Iteration 233, the loss is 205.79842938742152, parameters k is 41.108231830273795 and b is -30.018971652235233\n",
      "Iteration 234, the loss is 205.39346309359496, parameters k is 41.04538548640028 and b is -30.028971652235235\n",
      "Iteration 235, the loss is 204.98849679976817, parameters k is 40.98253914252676 and b is -30.038971652235237\n",
      "Iteration 236, the loss is 204.58353050594133, parameters k is 40.919692798653244 and b is -30.048971652235238\n",
      "Iteration 237, the loss is 204.1785642121143, parameters k is 40.85684645477973 and b is -30.05897165223524\n",
      "Iteration 238, the loss is 203.7735979182875, parameters k is 40.79400011090621 and b is -30.06897165223524\n",
      "Iteration 239, the loss is 203.36863162446073, parameters k is 40.731153767032694 and b is -30.078971652235243\n",
      "Iteration 240, the loss is 202.963665330634, parameters k is 40.66830742315918 and b is -30.088971652235244\n",
      "Iteration 241, the loss is 202.55869903680698, parameters k is 40.60546107928566 and b is -30.098971652235246\n",
      "Iteration 242, the loss is 202.15373274298025, parameters k is 40.542614735412144 and b is -30.108971652235248\n",
      "Iteration 243, the loss is 201.74876644915338, parameters k is 40.47976839153863 and b is -30.11897165223525\n",
      "Iteration 244, the loss is 201.34380015532642, parameters k is 40.41692204766511 and b is -30.12897165223525\n",
      "Iteration 245, the loss is 200.93883386149966, parameters k is 40.354075703791594 and b is -30.138971652235252\n",
      "Iteration 246, the loss is 200.53386756767276, parameters k is 40.29122935991808 and b is -30.148971652235254\n",
      "Iteration 247, the loss is 200.12890127384577, parameters k is 40.22838301604456 and b is -30.158971652235255\n",
      "Iteration 248, the loss is 199.7239349800192, parameters k is 40.165536672171044 and b is -30.168971652235257\n",
      "Iteration 249, the loss is 199.31896868619225, parameters k is 40.10269032829753 and b is -30.17897165223526\n",
      "Iteration 250, the loss is 198.91400239236555, parameters k is 40.03984398442401 and b is -30.18897165223526\n",
      "Iteration 251, the loss is 198.5090360985385, parameters k is 39.97699764055049 and b is -30.19897165223526\n",
      "Iteration 252, the loss is 198.10406980471197, parameters k is 39.91415129667698 and b is -30.208971652235263\n",
      "Iteration 253, the loss is 197.69910351088473, parameters k is 39.85130495280346 and b is -30.218971652235265\n",
      "Iteration 254, the loss is 197.29413721705825, parameters k is 39.78845860892994 and b is -30.228971652235266\n",
      "Iteration 255, the loss is 196.8891709232313, parameters k is 39.72561226505643 and b is -30.238971652235268\n",
      "Iteration 256, the loss is 196.4842046294043, parameters k is 39.66276592118291 and b is -30.24897165223527\n",
      "Iteration 257, the loss is 196.07923833557757, parameters k is 39.59991957730939 and b is -30.25897165223527\n",
      "Iteration 258, the loss is 195.6742720417507, parameters k is 39.537073233435876 and b is -30.268971652235273\n",
      "Iteration 259, the loss is 195.26930574792377, parameters k is 39.47422688956236 and b is -30.278971652235274\n",
      "Iteration 260, the loss is 194.86433945409698, parameters k is 39.41138054568884 and b is -30.288971652235276\n",
      "Iteration 261, the loss is 194.4593731602701, parameters k is 39.348534201815326 and b is -30.298971652235277\n",
      "Iteration 262, the loss is 194.05440686644337, parameters k is 39.28568785794181 and b is -30.30897165223528\n",
      "Iteration 263, the loss is 193.6494405726165, parameters k is 39.22284151406829 and b is -30.31897165223528\n",
      "Iteration 264, the loss is 193.24447427878977, parameters k is 39.159995170194776 and b is -30.328971652235282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265, the loss is 192.839507984963, parameters k is 39.09714882632126 and b is -30.338971652235283\n",
      "Iteration 266, the loss is 192.4345416911359, parameters k is 39.03430248244774 and b is -30.348971652235285\n",
      "Iteration 267, the loss is 192.02957539730912, parameters k is 38.971456138574226 and b is -30.358971652235287\n",
      "Iteration 268, the loss is 191.6246091034825, parameters k is 38.90860979470071 and b is -30.368971652235288\n",
      "Iteration 269, the loss is 191.21964280965543, parameters k is 38.84576345082719 and b is -30.37897165223529\n",
      "Iteration 270, the loss is 190.81467651582875, parameters k is 38.782917106953676 and b is -30.38897165223529\n",
      "Iteration 271, the loss is 190.40971022200176, parameters k is 38.72007076308016 and b is -30.398971652235293\n",
      "Iteration 272, the loss is 190.0047439281747, parameters k is 38.65722441920664 and b is -30.408971652235294\n",
      "Iteration 273, the loss is 189.59977763434804, parameters k is 38.594378075333125 and b is -30.418971652235296\n",
      "Iteration 274, the loss is 189.19481134052123, parameters k is 38.53153173145961 and b is -30.428971652235298\n",
      "Iteration 275, the loss is 188.78984504669432, parameters k is 38.46868538758609 and b is -30.4389716522353\n",
      "Iteration 276, the loss is 188.38487875286776, parameters k is 38.405839043712575 and b is -30.4489716522353\n",
      "Iteration 277, the loss is 187.97991245904075, parameters k is 38.34299269983906 and b is -30.458971652235302\n",
      "Iteration 278, the loss is 187.57494616521376, parameters k is 38.28014635596554 and b is -30.468971652235304\n",
      "Iteration 279, the loss is 187.16997987138706, parameters k is 38.217300012092025 and b is -30.478971652235305\n",
      "Iteration 280, the loss is 186.76501357756024, parameters k is 38.15445366821851 and b is -30.488971652235307\n",
      "Iteration 281, the loss is 186.36004728373314, parameters k is 38.09160732434499 and b is -30.49897165223531\n",
      "Iteration 282, the loss is 185.9550809899064, parameters k is 38.028760980471475 and b is -30.50897165223531\n",
      "Iteration 283, the loss is 185.55011469607982, parameters k is 37.96591463659796 and b is -30.51897165223531\n",
      "Iteration 284, the loss is 185.14514840225294, parameters k is 37.90306829272444 and b is -30.528971652235313\n",
      "Iteration 285, the loss is 184.74018210842624, parameters k is 37.840221948850925 and b is -30.538971652235315\n",
      "Iteration 286, the loss is 184.33521581459917, parameters k is 37.77737560497741 and b is -30.548971652235316\n",
      "Iteration 287, the loss is 183.93024952077235, parameters k is 37.71452926110389 and b is -30.558971652235318\n",
      "Iteration 288, the loss is 183.52528322694565, parameters k is 37.651682917230374 and b is -30.56897165223532\n",
      "Iteration 289, the loss is 183.1203169331187, parameters k is 37.58883657335686 and b is -30.57897165223532\n",
      "Iteration 290, the loss is 182.71535063929187, parameters k is 37.52599022948334 and b is -30.588971652235323\n",
      "Iteration 291, the loss is 182.31038434546517, parameters k is 37.463143885609824 and b is -30.598971652235324\n",
      "Iteration 292, the loss is 181.90541805163804, parameters k is 37.40029754173631 and b is -30.608971652235326\n",
      "Iteration 293, the loss is 181.5004517578113, parameters k is 37.33745119786279 and b is -30.618971652235327\n",
      "Iteration 294, the loss is 181.09548546398446, parameters k is 37.274604853989274 and b is -30.62897165223533\n",
      "Iteration 295, the loss is 180.6905191701577, parameters k is 37.21175851011576 and b is -30.63897165223533\n",
      "Iteration 296, the loss is 180.285552876331, parameters k is 37.14891216624224 and b is -30.648971652235332\n",
      "Iteration 297, the loss is 179.88058658250378, parameters k is 37.086065822368724 and b is -30.658971652235333\n",
      "Iteration 298, the loss is 179.47562028867725, parameters k is 37.02321947849521 and b is -30.668971652235335\n",
      "Iteration 299, the loss is 179.0706539948505, parameters k is 36.96037313462169 and b is -30.678971652235337\n",
      "Iteration 300, the loss is 178.66568770102347, parameters k is 36.897526790748174 and b is -30.688971652235338\n",
      "Iteration 301, the loss is 178.26072140719663, parameters k is 36.83468044687466 and b is -30.69897165223534\n",
      "Iteration 302, the loss is 177.85575511336984, parameters k is 36.77183410300114 and b is -30.70897165223534\n",
      "Iteration 303, the loss is 177.45078881954305, parameters k is 36.70898775912762 and b is -30.718971652235343\n",
      "Iteration 304, the loss is 177.04582252571603, parameters k is 36.64614141525411 and b is -30.728971652235344\n",
      "Iteration 305, the loss is 176.6408562318893, parameters k is 36.58329507138059 and b is -30.738971652235346\n",
      "Iteration 306, the loss is 176.23588993806248, parameters k is 36.52044872750707 and b is -30.748971652235348\n",
      "Iteration 307, the loss is 175.83092364423575, parameters k is 36.45760238363356 and b is -30.75897165223535\n",
      "Iteration 308, the loss is 175.4259573504088, parameters k is 36.39475603976004 and b is -30.76897165223535\n",
      "Iteration 309, the loss is 175.020991056582, parameters k is 36.33190969588652 and b is -30.778971652235352\n",
      "Iteration 310, the loss is 174.6160247627552, parameters k is 36.269063352013006 and b is -30.788971652235354\n",
      "Iteration 311, the loss is 174.21105846892814, parameters k is 36.20621700813949 and b is -30.798971652235355\n",
      "Iteration 312, the loss is 173.80609217510144, parameters k is 36.14337066426597 and b is -30.808971652235357\n",
      "Iteration 313, the loss is 173.40112588127468, parameters k is 36.080524320392456 and b is -30.81897165223536\n",
      "Iteration 314, the loss is 172.99615958744783, parameters k is 36.01767797651894 and b is -30.82897165223536\n",
      "Iteration 315, the loss is 172.5911932936209, parameters k is 35.95483163264542 and b is -30.83897165223536\n",
      "Iteration 316, the loss is 172.18622699979394, parameters k is 35.891985288771906 and b is -30.848971652235363\n",
      "Iteration 317, the loss is 171.78126070596727, parameters k is 35.82913894489839 and b is -30.858971652235365\n",
      "Iteration 318, the loss is 171.3762944121404, parameters k is 35.76629260102487 and b is -30.868971652235366\n",
      "Iteration 319, the loss is 170.97132811831366, parameters k is 35.703446257151356 and b is -30.878971652235368\n",
      "Iteration 320, the loss is 170.56636182448668, parameters k is 35.64059991327784 and b is -30.88897165223537\n",
      "Iteration 321, the loss is 170.16139553066003, parameters k is 35.57775356940432 and b is -30.89897165223537\n",
      "Iteration 322, the loss is 169.75642923683316, parameters k is 35.514907225530806 and b is -30.908971652235373\n",
      "Iteration 323, the loss is 169.35146294300628, parameters k is 35.45206088165729 and b is -30.918971652235374\n",
      "Iteration 324, the loss is 168.94649664917938, parameters k is 35.38921453778377 and b is -30.928971652235376\n",
      "Iteration 325, the loss is 168.54153035535248, parameters k is 35.326368193910255 and b is -30.938971652235377\n",
      "Iteration 326, the loss is 168.13656406152563, parameters k is 35.26352185003674 and b is -30.94897165223538\n",
      "Iteration 327, the loss is 167.73159776769887, parameters k is 35.20067550616322 and b is -30.95897165223538\n",
      "Iteration 328, the loss is 167.32663147387177, parameters k is 35.137829162289705 and b is -30.968971652235382\n",
      "Iteration 329, the loss is 166.92166518004512, parameters k is 35.07498281841619 and b is -30.978971652235384\n",
      "Iteration 330, the loss is 166.51669888621825, parameters k is 35.01213647454267 and b is -30.988971652235385\n",
      "Iteration 331, the loss is 166.1117325923916, parameters k is 34.949290130669155 and b is -30.998971652235387\n",
      "Iteration 332, the loss is 165.70676629856473, parameters k is 34.88644378679564 and b is -31.008971652235388\n",
      "Iteration 333, the loss is 165.30180000473766, parameters k is 34.82359744292212 and b is -31.01897165223539\n",
      "Iteration 334, the loss is 164.89683371091098, parameters k is 34.760751099048605 and b is -31.02897165223539\n",
      "Iteration 335, the loss is 164.4918674170842, parameters k is 34.69790475517509 and b is -31.038971652235393\n",
      "Iteration 336, the loss is 164.0869011232573, parameters k is 34.63505841130157 and b is -31.048971652235394\n",
      "Iteration 337, the loss is 163.68193482943025, parameters k is 34.572212067428055 and b is -31.058971652235396\n",
      "Iteration 338, the loss is 163.27696853560354, parameters k is 34.50936572355454 and b is -31.068971652235398\n",
      "Iteration 339, the loss is 162.87200224177676, parameters k is 34.44651937968102 and b is -31.0789716522354\n",
      "Iteration 340, the loss is 162.46703594794982, parameters k is 34.383673035807504 and b is -31.0889716522354\n",
      "Iteration 341, the loss is 162.06206965412306, parameters k is 34.32082669193399 and b is -31.098971652235402\n",
      "Iteration 342, the loss is 161.65710336029605, parameters k is 34.25798034806047 and b is -31.108971652235404\n",
      "Iteration 343, the loss is 161.25213706646932, parameters k is 34.195134004186954 and b is -31.118971652235405\n",
      "Iteration 344, the loss is 160.84717077264241, parameters k is 34.13228766031344 and b is -31.128971652235407\n",
      "Iteration 345, the loss is 160.4422044788159, parameters k is 34.06944131643992 and b is -31.13897165223541\n",
      "Iteration 346, the loss is 160.0372381849888, parameters k is 34.006594972566404 and b is -31.14897165223541\n",
      "Iteration 347, the loss is 159.63227189116188, parameters k is 33.94374862869289 and b is -31.15897165223541\n",
      "Iteration 348, the loss is 159.2273055973351, parameters k is 33.88090228481937 and b is -31.168971652235413\n",
      "Iteration 349, the loss is 158.8223393035083, parameters k is 33.818055940945854 and b is -31.178971652235415\n",
      "Iteration 350, the loss is 158.4173730096815, parameters k is 33.75520959707234 and b is -31.188971652235416\n",
      "Iteration 351, the loss is 158.0124067158547, parameters k is 33.69236325319882 and b is -31.198971652235418\n",
      "Iteration 352, the loss is 157.6074404220279, parameters k is 33.629516909325304 and b is -31.20897165223542\n",
      "Iteration 353, the loss is 157.20247412820106, parameters k is 33.56667056545179 and b is -31.21897165223542\n",
      "Iteration 354, the loss is 156.7975078343741, parameters k is 33.50382422157827 and b is -31.228971652235423\n",
      "Iteration 355, the loss is 156.3925415405472, parameters k is 33.44097787770475 and b is -31.238971652235424\n",
      "Iteration 356, the loss is 155.9875752467204, parameters k is 33.37813153383124 and b is -31.248971652235426\n",
      "Iteration 357, the loss is 155.58260895289357, parameters k is 33.31528518995772 and b is -31.258971652235427\n",
      "Iteration 358, the loss is 155.17764265906692, parameters k is 33.2524388460842 and b is -31.26897165223543\n",
      "Iteration 359, the loss is 154.7726763652401, parameters k is 33.18959250221069 and b is -31.27897165223543\n",
      "Iteration 360, the loss is 154.36771007141317, parameters k is 33.12674615833717 and b is -31.288971652235432\n",
      "Iteration 361, the loss is 153.9627437775863, parameters k is 33.06389981446365 and b is -31.298971652235434\n",
      "Iteration 362, the loss is 153.55777748375928, parameters k is 33.001053470590136 and b is -31.308971652235435\n",
      "Iteration 363, the loss is 153.1528111899325, parameters k is 32.93820712671662 and b is -31.318971652235437\n",
      "Iteration 364, the loss is 152.74784489610573, parameters k is 32.8753607828431 and b is -31.32897165223544\n",
      "Iteration 365, the loss is 152.34287860227886, parameters k is 32.812514438969586 and b is -31.33897165223544\n",
      "Iteration 366, the loss is 151.93791230845204, parameters k is 32.74966809509607 and b is -31.34897165223544\n",
      "Iteration 367, the loss is 151.5329460146252, parameters k is 32.68682175122255 and b is -31.358971652235443\n",
      "Iteration 368, the loss is 151.12797972079855, parameters k is 32.623975407349036 and b is -31.368971652235444\n",
      "Iteration 369, the loss is 150.72301342697156, parameters k is 32.56112906347552 and b is -31.378971652235446\n",
      "Iteration 370, the loss is 150.31804713314477, parameters k is 32.498282719602 and b is -31.388971652235448\n",
      "Iteration 371, the loss is 149.91308083931798, parameters k is 32.435436375728486 and b is -31.39897165223545\n",
      "Iteration 372, the loss is 149.50811454549094, parameters k is 32.37259003185497 and b is -31.40897165223545\n",
      "Iteration 373, the loss is 149.10314825166427, parameters k is 32.30974368798145 and b is -31.418971652235452\n",
      "Iteration 374, the loss is 148.6981819578372, parameters k is 32.246897344107936 and b is -31.428971652235454\n",
      "Iteration 375, the loss is 148.2932156640105, parameters k is 32.18405100023442 and b is -31.438971652235455\n",
      "Iteration 376, the loss is 147.8882493701836, parameters k is 32.1212046563609 and b is -31.448971652235457\n",
      "Iteration 377, the loss is 147.48328307635677, parameters k is 32.058358312487385 and b is -31.45897165223546\n",
      "Iteration 378, the loss is 147.07831678253, parameters k is 31.99551196861387 and b is -31.46897165223546\n",
      "Iteration 379, the loss is 146.67335048870305, parameters k is 31.932665624740352 and b is -31.47897165223546\n",
      "Iteration 380, the loss is 146.2683841948764, parameters k is 31.869819280866835 and b is -31.488971652235463\n",
      "Iteration 381, the loss is 145.8634179010495, parameters k is 31.80697293699332 and b is -31.498971652235465\n",
      "Iteration 382, the loss is 145.45845160722274, parameters k is 31.744126593119802 and b is -31.508971652235466\n",
      "Iteration 383, the loss is 145.05348531339595, parameters k is 31.681280249246285 and b is -31.518971652235468\n",
      "Iteration 384, the loss is 144.64851901956905, parameters k is 31.61843390537277 and b is -31.52897165223547\n",
      "Iteration 385, the loss is 144.2435527257423, parameters k is 31.55558756149925 and b is -31.53897165223547\n",
      "Iteration 386, the loss is 143.83858643191536, parameters k is 31.492741217625735 and b is -31.548971652235473\n",
      "Iteration 387, the loss is 143.43362013808851, parameters k is 31.429894873752218 and b is -31.558971652235474\n",
      "Iteration 388, the loss is 143.02865384426158, parameters k is 31.3670485298787 and b is -31.568971652235476\n",
      "Iteration 389, the loss is 142.62368755043468, parameters k is 31.304202186005185 and b is -31.578971652235477\n",
      "Iteration 390, the loss is 142.2187212566079, parameters k is 31.241355842131668 and b is -31.58897165223548\n",
      "Iteration 391, the loss is 141.81375496278096, parameters k is 31.17850949825815 and b is -31.59897165223548\n",
      "Iteration 392, the loss is 141.40878866895423, parameters k is 31.115663154384634 and b is -31.608971652235482\n",
      "Iteration 393, the loss is 141.00382237512733, parameters k is 31.052816810511118 and b is -31.618971652235484\n",
      "Iteration 394, the loss is 140.59885608130054, parameters k is 30.9899704666376 and b is -31.628971652235485\n",
      "Iteration 395, the loss is 140.1938897874736, parameters k is 30.927124122764084 and b is -31.638971652235487\n",
      "Iteration 396, the loss is 139.78892349364688, parameters k is 30.864277778890568 and b is -31.64897165223549\n",
      "Iteration 397, the loss is 139.38395719981997, parameters k is 30.80143143501705 and b is -31.65897165223549\n",
      "Iteration 398, the loss is 138.9789909059932, parameters k is 30.738585091143534 and b is -31.66897165223549\n",
      "Iteration 399, the loss is 138.5740246121663, parameters k is 30.675738747270017 and b is -31.678971652235493\n",
      "Iteration 400, the loss is 138.1690583183395, parameters k is 30.6128924033965 and b is -31.688971652235495\n",
      "Iteration 401, the loss is 137.76409202451268, parameters k is 30.550046059522984 and b is -31.698971652235496\n",
      "Iteration 402, the loss is 137.35912573068583, parameters k is 30.487199715649467 and b is -31.708971652235498\n",
      "Iteration 403, the loss is 136.95415943685887, parameters k is 30.42435337177595 and b is -31.7189716522355\n",
      "Iteration 404, the loss is 136.54919314303214, parameters k is 30.361507027902434 and b is -31.7289716522355\n",
      "Iteration 405, the loss is 136.14422684920518, parameters k is 30.298660684028917 and b is -31.738971652235502\n",
      "Iteration 406, the loss is 135.73926055537848, parameters k is 30.2358143401554 and b is -31.748971652235504\n",
      "Iteration 407, the loss is 135.33429426155158, parameters k is 30.172967996281884 and b is -31.758971652235505\n",
      "Iteration 408, the loss is 134.92932796772487, parameters k is 30.110121652408367 and b is -31.768971652235507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 409, the loss is 134.524361673898, parameters k is 30.04727530853485 and b is -31.77897165223551\n",
      "Iteration 410, the loss is 134.11939538007104, parameters k is 29.984428964661333 and b is -31.78897165223551\n",
      "Iteration 411, the loss is 133.7144290862443, parameters k is 29.921582620787817 and b is -31.79897165223551\n",
      "Iteration 412, the loss is 133.30946279241735, parameters k is 29.8587362769143 and b is -31.808971652235513\n",
      "Iteration 413, the loss is 132.90449649859062, parameters k is 29.795889933040783 and b is -31.818971652235515\n",
      "Iteration 414, the loss is 132.49953020476372, parameters k is 29.733043589167266 and b is -31.828971652235516\n",
      "Iteration 415, the loss is 132.0945639109368, parameters k is 29.67019724529375 and b is -31.838971652235518\n",
      "Iteration 416, the loss is 131.68959761711005, parameters k is 29.607350901420233 and b is -31.84897165223552\n",
      "Iteration 417, the loss is 131.28463132328318, parameters k is 29.544504557546716 and b is -31.85897165223552\n",
      "Iteration 418, the loss is 130.87966502945645, parameters k is 29.4816582136732 and b is -31.868971652235523\n",
      "Iteration 419, the loss is 130.47469873562954, parameters k is 29.418811869799683 and b is -31.878971652235524\n",
      "Iteration 420, the loss is 130.06973244180264, parameters k is 29.355965525926166 and b is -31.888971652235526\n",
      "Iteration 421, the loss is 129.66476614797585, parameters k is 29.29311918205265 and b is -31.898971652235527\n",
      "Iteration 422, the loss is 129.25979985414898, parameters k is 29.230272838179133 and b is -31.90897165223553\n",
      "Iteration 423, the loss is 128.85483356032208, parameters k is 29.167426494305616 and b is -31.91897165223553\n",
      "Iteration 424, the loss is 128.44986726649535, parameters k is 29.1045801504321 and b is -31.928971652235532\n",
      "Iteration 425, the loss is 128.04490097266844, parameters k is 29.041733806558582 and b is -31.938971652235534\n",
      "Iteration 426, the loss is 127.63993467884168, parameters k is 28.978887462685066 and b is -31.948971652235535\n",
      "Iteration 427, the loss is 127.23496838501488, parameters k is 28.91604111881155 and b is -31.958971652235537\n",
      "Iteration 428, the loss is 126.83000209118795, parameters k is 28.853194774938032 and b is -31.96897165223554\n",
      "Iteration 429, the loss is 126.4250357973611, parameters k is 28.790348431064515 and b is -31.97897165223554\n",
      "Iteration 430, the loss is 126.02006950353442, parameters k is 28.727502087191 and b is -31.98897165223554\n",
      "Iteration 431, the loss is 125.61510320970754, parameters k is 28.664655743317482 and b is -31.998971652235543\n",
      "Iteration 432, the loss is 125.2101369158807, parameters k is 28.601809399443965 and b is -32.008971652235545\n",
      "Iteration 433, the loss is 124.80517062205391, parameters k is 28.53896305557045 and b is -32.01897165223554\n",
      "Iteration 434, the loss is 124.40020432822703, parameters k is 28.476116711696932 and b is -32.02897165223554\n",
      "Iteration 435, the loss is 123.99523803440017, parameters k is 28.413270367823415 and b is -32.03897165223554\n",
      "Iteration 436, the loss is 123.59027174057331, parameters k is 28.3504240239499 and b is -32.04897165223554\n",
      "Iteration 437, the loss is 123.18530544674651, parameters k is 28.28757768007638 and b is -32.058971652235535\n",
      "Iteration 438, the loss is 122.78033915291962, parameters k is 28.224731336202865 and b is -32.06897165223553\n",
      "Iteration 439, the loss is 122.37537285909269, parameters k is 28.161884992329348 and b is -32.07897165223553\n",
      "Iteration 440, the loss is 121.97040656526599, parameters k is 28.09903864845583 and b is -32.08897165223553\n",
      "Iteration 441, the loss is 121.56544027143913, parameters k is 28.036192304582315 and b is -32.09897165223553\n",
      "Iteration 442, the loss is 121.16047397761227, parameters k is 27.973345960708798 and b is -32.108971652235525\n",
      "Iteration 443, the loss is 120.75550768378552, parameters k is 27.91049961683528 and b is -32.11897165223552\n",
      "Iteration 444, the loss is 120.35054138995876, parameters k is 27.847653272961765 and b is -32.12897165223552\n",
      "Iteration 445, the loss is 119.94557509613169, parameters k is 27.784806929088248 and b is -32.13897165223552\n",
      "Iteration 446, the loss is 119.54060880230489, parameters k is 27.72196058521473 and b is -32.14897165223552\n",
      "Iteration 447, the loss is 119.1356425084781, parameters k is 27.659114241341214 and b is -32.158971652235515\n",
      "Iteration 448, the loss is 118.73067621465134, parameters k is 27.596267897467698 and b is -32.16897165223551\n",
      "Iteration 449, the loss is 118.32570992082458, parameters k is 27.53342155359418 and b is -32.17897165223551\n",
      "Iteration 450, the loss is 117.92074362699755, parameters k is 27.470575209720664 and b is -32.18897165223551\n",
      "Iteration 451, the loss is 117.5157773331708, parameters k is 27.407728865847147 and b is -32.19897165223551\n",
      "Iteration 452, the loss is 117.11081103934404, parameters k is 27.34488252197363 and b is -32.208971652235505\n",
      "Iteration 453, the loss is 116.70584474551707, parameters k is 27.282036178100114 and b is -32.2189716522355\n",
      "Iteration 454, the loss is 116.30087845169027, parameters k is 27.219189834226597 and b is -32.2289716522355\n",
      "Iteration 455, the loss is 115.89591215786342, parameters k is 27.15634349035308 and b is -32.2389716522355\n",
      "Iteration 456, the loss is 115.4909458640366, parameters k is 27.093497146479564 and b is -32.2489716522355\n",
      "Iteration 457, the loss is 115.08597957020984, parameters k is 27.030650802606047 and b is -32.258971652235495\n",
      "Iteration 458, the loss is 114.68101327638293, parameters k is 26.96780445873253 and b is -32.26897165223549\n",
      "Iteration 459, the loss is 114.27604698255611, parameters k is 26.904958114859014 and b is -32.27897165223549\n",
      "Iteration 460, the loss is 113.87108068872926, parameters k is 26.842111770985497 and b is -32.28897165223549\n",
      "Iteration 461, the loss is 113.46611439490236, parameters k is 26.77926542711198 and b is -32.29897165223549\n",
      "Iteration 462, the loss is 113.06114810107559, parameters k is 26.716419083238463 and b is -32.308971652235485\n",
      "Iteration 463, the loss is 112.65618180724873, parameters k is 26.653572739364947 and b is -32.31897165223548\n",
      "Iteration 464, the loss is 112.25121551342181, parameters k is 26.59072639549143 and b is -32.32897165223548\n",
      "Iteration 465, the loss is 111.84624921959505, parameters k is 26.527880051617913 and b is -32.33897165223548\n",
      "Iteration 466, the loss is 111.44128292576828, parameters k is 26.465033707744396 and b is -32.34897165223548\n",
      "Iteration 467, the loss is 111.03631663194132, parameters k is 26.40218736387088 and b is -32.358971652235475\n",
      "Iteration 468, the loss is 110.63135033811454, parameters k is 26.339341019997363 and b is -32.36897165223547\n",
      "Iteration 469, the loss is 110.2263840442878, parameters k is 26.276494676123846 and b is -32.37897165223547\n",
      "Iteration 470, the loss is 109.82141775046082, parameters k is 26.21364833225033 and b is -32.38897165223547\n",
      "Iteration 471, the loss is 109.41645145663401, parameters k is 26.150801988376813 and b is -32.39897165223547\n",
      "Iteration 472, the loss is 109.01148516280716, parameters k is 26.087955644503296 and b is -32.408971652235465\n",
      "Iteration 473, the loss is 108.6065188689804, parameters k is 26.02510930062978 and b is -32.41897165223546\n",
      "Iteration 474, the loss is 108.20155257515358, parameters k is 25.962262956756263 and b is -32.42897165223546\n",
      "Iteration 475, the loss is 107.79658628132675, parameters k is 25.899416612882746 and b is -32.43897165223546\n",
      "Iteration 476, the loss is 107.39161998749975, parameters k is 25.83657026900923 and b is -32.44897165223546\n",
      "Iteration 477, the loss is 106.98665369367295, parameters k is 25.773723925135712 and b is -32.458971652235455\n",
      "Iteration 478, the loss is 106.58168739984615, parameters k is 25.710877581262196 and b is -32.46897165223545\n",
      "Iteration 479, the loss is 106.1767211060194, parameters k is 25.64803123738868 and b is -32.47897165223545\n",
      "Iteration 480, the loss is 105.77175481219257, parameters k is 25.585184893515162 and b is -32.48897165223545\n",
      "Iteration 481, the loss is 105.36678851836555, parameters k is 25.522338549641645 and b is -32.49897165223545\n",
      "Iteration 482, the loss is 104.96182222453889, parameters k is 25.45949220576813 and b is -32.508971652235445\n",
      "Iteration 483, the loss is 104.55685593071206, parameters k is 25.396645861894612 and b is -32.51897165223544\n",
      "Iteration 484, the loss is 104.15188963688516, parameters k is 25.333799518021095 and b is -32.52897165223544\n",
      "Iteration 485, the loss is 103.7469233430583, parameters k is 25.27095317414758 and b is -32.53897165223544\n",
      "Iteration 486, the loss is 103.34195704923138, parameters k is 25.208106830274062 and b is -32.54897165223544\n",
      "Iteration 487, the loss is 102.93699075540472, parameters k is 25.145260486400545 and b is -32.558971652235435\n",
      "Iteration 488, the loss is 102.53202446157778, parameters k is 25.08241414252703 and b is -32.56897165223543\n",
      "Iteration 489, the loss is 102.1270581677509, parameters k is 25.01956779865351 and b is -32.57897165223543\n",
      "Iteration 490, the loss is 101.72209187392417, parameters k is 24.956721454779995 and b is -32.58897165223543\n",
      "Iteration 491, the loss is 101.31712558009731, parameters k is 24.893875110906478 and b is -32.59897165223543\n",
      "Iteration 492, the loss is 100.9121592862705, parameters k is 24.83102876703296 and b is -32.608971652235425\n",
      "Iteration 493, the loss is 100.50719299244368, parameters k is 24.768182423159445 and b is -32.61897165223542\n",
      "Iteration 494, the loss is 100.10222669861679, parameters k is 24.705336079285928 and b is -32.62897165223542\n",
      "Iteration 495, the loss is 99.69726040478994, parameters k is 24.64248973541241 and b is -32.63897165223542\n",
      "Iteration 496, the loss is 99.29229411096307, parameters k is 24.579643391538895 and b is -32.64897165223542\n",
      "Iteration 497, the loss is 98.88732781713627, parameters k is 24.516797047665378 and b is -32.658971652235415\n",
      "Iteration 498, the loss is 98.48236152330945, parameters k is 24.45395070379186 and b is -32.66897165223541\n",
      "Iteration 499, the loss is 98.07739522948258, parameters k is 24.391104359918344 and b is -32.67897165223541\n",
      "Iteration 500, the loss is 97.6724289356557, parameters k is 24.328258016044828 and b is -32.68897165223541\n",
      "Iteration 501, the loss is 97.26746264182898, parameters k is 24.26541167217131 and b is -32.69897165223541\n",
      "Iteration 502, the loss is 96.86249634800198, parameters k is 24.202565328297794 and b is -32.708971652235405\n",
      "Iteration 503, the loss is 96.45753005417524, parameters k is 24.139718984424277 and b is -32.7189716522354\n",
      "Iteration 504, the loss is 96.05256376034853, parameters k is 24.07687264055076 and b is -32.7289716522354\n",
      "Iteration 505, the loss is 95.64759746652163, parameters k is 24.014026296677244 and b is -32.7389716522354\n",
      "Iteration 506, the loss is 95.2426311726948, parameters k is 23.951179952803727 and b is -32.7489716522354\n",
      "Iteration 507, the loss is 94.83766487886797, parameters k is 23.88833360893021 and b is -32.758971652235395\n",
      "Iteration 508, the loss is 94.43269858504117, parameters k is 23.825487265056694 and b is -32.76897165223539\n",
      "Iteration 509, the loss is 94.02773229121425, parameters k is 23.762640921183177 and b is -32.77897165223539\n",
      "Iteration 510, the loss is 93.6227659973874, parameters k is 23.69979457730966 and b is -32.78897165223539\n",
      "Iteration 511, the loss is 93.21779970356054, parameters k is 23.636948233436144 and b is -32.79897165223539\n",
      "Iteration 512, the loss is 92.81283340973374, parameters k is 23.574101889562627 and b is -32.808971652235385\n",
      "Iteration 513, the loss is 92.40786711590705, parameters k is 23.51125554568911 and b is -32.81897165223538\n",
      "Iteration 514, the loss is 92.0029008220801, parameters k is 23.448409201815593 and b is -32.82897165223538\n",
      "Iteration 515, the loss is 91.5979345282532, parameters k is 23.385562857942077 and b is -32.83897165223538\n",
      "Iteration 516, the loss is 91.19296823442642, parameters k is 23.32271651406856 and b is -32.84897165223538\n",
      "Iteration 517, the loss is 90.78800194059949, parameters k is 23.259870170195043 and b is -32.858971652235375\n",
      "Iteration 518, the loss is 90.38303564677275, parameters k is 23.197023826321526 and b is -32.86897165223537\n",
      "Iteration 519, the loss is 89.97806935294584, parameters k is 23.13417748244801 and b is -32.87897165223537\n",
      "Iteration 520, the loss is 89.57310305911903, parameters k is 23.071331138574493 and b is -32.88897165223537\n",
      "Iteration 521, the loss is 89.16813676529225, parameters k is 23.008484794700976 and b is -32.89897165223537\n",
      "Iteration 522, the loss is 88.76317047146537, parameters k is 22.94563845082746 and b is -32.908971652235365\n",
      "Iteration 523, the loss is 88.35820417763858, parameters k is 22.882792106953943 and b is -32.91897165223536\n",
      "Iteration 524, the loss is 87.95323788381178, parameters k is 22.819945763080426 and b is -32.92897165223536\n",
      "Iteration 525, the loss is 87.54827158998496, parameters k is 22.75709941920691 and b is -32.93897165223536\n",
      "Iteration 526, the loss is 87.14330529615806, parameters k is 22.694253075333393 and b is -32.94897165223536\n",
      "Iteration 527, the loss is 86.73833900233122, parameters k is 22.631406731459876 and b is -32.958971652235356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 528, the loss is 86.33337270850437, parameters k is 22.56856038758636 and b is -32.96897165223535\n",
      "Iteration 529, the loss is 85.92840641467755, parameters k is 22.505714043712842 and b is -32.97897165223535\n",
      "Iteration 530, the loss is 85.52344012085074, parameters k is 22.442867699839326 and b is -32.98897165223535\n",
      "Iteration 531, the loss is 85.11847382702399, parameters k is 22.38002135596581 and b is -32.99897165223535\n",
      "Iteration 532, the loss is 84.71350753319707, parameters k is 22.317175012092292 and b is -33.008971652235346\n",
      "Iteration 533, the loss is 84.30854123937023, parameters k is 22.254328668218776 and b is -33.018971652235344\n",
      "Iteration 534, the loss is 83.90357494554337, parameters k is 22.19148232434526 and b is -33.02897165223534\n",
      "Iteration 535, the loss is 83.49860865171647, parameters k is 22.128635980471742 and b is -33.03897165223534\n",
      "Iteration 536, the loss is 83.0936423578897, parameters k is 22.065789636598225 and b is -33.04897165223534\n",
      "Iteration 537, the loss is 82.68867606406292, parameters k is 22.00294329272471 and b is -33.058971652235336\n",
      "Iteration 538, the loss is 82.28370977023604, parameters k is 21.940096948851192 and b is -33.068971652235334\n",
      "Iteration 539, the loss is 81.87874347640917, parameters k is 21.877250604977675 and b is -33.07897165223533\n",
      "Iteration 540, the loss is 81.47377718258235, parameters k is 21.81440426110416 and b is -33.08897165223533\n",
      "Iteration 541, the loss is 81.06881088875552, parameters k is 21.75155791723064 and b is -33.09897165223533\n",
      "Iteration 542, the loss is 80.66384459492878, parameters k is 21.688711573357125 and b is -33.108971652235326\n",
      "Iteration 543, the loss is 80.25887830110189, parameters k is 21.62586522948361 and b is -33.118971652235324\n",
      "Iteration 544, the loss is 79.85391200727507, parameters k is 21.56301888561009 and b is -33.12897165223532\n",
      "Iteration 545, the loss is 79.4489457134481, parameters k is 21.500172541736575 and b is -33.13897165223532\n",
      "Iteration 546, the loss is 79.04397941962132, parameters k is 21.437326197863058 and b is -33.14897165223532\n",
      "Iteration 547, the loss is 78.6390131257945, parameters k is 21.37447985398954 and b is -33.158971652235316\n",
      "Iteration 548, the loss is 78.23404683196766, parameters k is 21.311633510116025 and b is -33.168971652235314\n",
      "Iteration 549, the loss is 77.8290805381408, parameters k is 21.248787166242508 and b is -33.17897165223531\n",
      "Iteration 550, the loss is 77.42411424431403, parameters k is 21.18594082236899 and b is -33.18897165223531\n",
      "Iteration 551, the loss is 77.01914795048701, parameters k is 21.123094478495474 and b is -33.19897165223531\n",
      "Iteration 552, the loss is 76.6141816566604, parameters k is 21.060248134621958 and b is -33.208971652235306\n",
      "Iteration 553, the loss is 76.20921536283345, parameters k is 20.99740179074844 and b is -33.218971652235304\n",
      "Iteration 554, the loss is 75.80424906900672, parameters k is 20.934555446874924 and b is -33.2289716522353\n",
      "Iteration 555, the loss is 75.39928277517987, parameters k is 20.871709103001407 and b is -33.2389716522353\n",
      "Iteration 556, the loss is 74.99431648135301, parameters k is 20.80886275912789 and b is -33.2489716522353\n",
      "Iteration 557, the loss is 74.58935018752628, parameters k is 20.746016415254374 and b is -33.258971652235296\n",
      "Iteration 558, the loss is 74.18438389369935, parameters k is 20.683170071380857 and b is -33.268971652235294\n",
      "Iteration 559, the loss is 73.77941759987243, parameters k is 20.62032372750734 and b is -33.27897165223529\n",
      "Iteration 560, the loss is 73.37445130604561, parameters k is 20.557477383633824 and b is -33.28897165223529\n",
      "Iteration 561, the loss is 72.96948501221883, parameters k is 20.494631039760307 and b is -33.29897165223529\n",
      "Iteration 562, the loss is 72.56451871839198, parameters k is 20.43178469588679 and b is -33.308971652235286\n",
      "Iteration 563, the loss is 72.15955242456508, parameters k is 20.368938352013274 and b is -33.318971652235284\n",
      "Iteration 564, the loss is 71.75458613073829, parameters k is 20.306092008139757 and b is -33.32897165223528\n",
      "Iteration 565, the loss is 71.34961983691147, parameters k is 20.24324566426624 and b is -33.33897165223528\n",
      "Iteration 566, the loss is 70.9446535430846, parameters k is 20.180399320392723 and b is -33.34897165223528\n",
      "Iteration 567, the loss is 70.53968724925792, parameters k is 20.117552976519207 and b is -33.358971652235276\n",
      "Iteration 568, the loss is 70.13472095543094, parameters k is 20.05470663264569 and b is -33.368971652235274\n",
      "Iteration 569, the loss is 69.72975466160405, parameters k is 19.991860288772173 and b is -33.37897165223527\n",
      "Iteration 570, the loss is 69.32478836777734, parameters k is 19.929013944898657 and b is -33.38897165223527\n",
      "Iteration 571, the loss is 68.91982207395043, parameters k is 19.86616760102514 and b is -33.39897165223527\n",
      "Iteration 572, the loss is 68.5148557801236, parameters k is 19.803321257151623 and b is -33.408971652235266\n",
      "Iteration 573, the loss is 68.10988948629678, parameters k is 19.740474913278106 and b is -33.418971652235264\n",
      "Iteration 574, the loss is 67.7049231924699, parameters k is 19.67762856940459 and b is -33.42897165223526\n",
      "Iteration 575, the loss is 67.29995689864307, parameters k is 19.614782225531073 and b is -33.43897165223526\n",
      "Iteration 576, the loss is 66.8949906048163, parameters k is 19.551935881657556 and b is -33.44897165223526\n",
      "Iteration 577, the loss is 66.49002431098943, parameters k is 19.48908953778404 and b is -33.458971652235256\n",
      "Iteration 578, the loss is 66.0850580171625, parameters k is 19.426243193910523 and b is -33.468971652235254\n",
      "Iteration 579, the loss is 65.6800917233358, parameters k is 19.363396850037006 and b is -33.47897165223525\n",
      "Iteration 580, the loss is 65.27512542950883, parameters k is 19.30055050616349 and b is -33.48897165223525\n",
      "Iteration 581, the loss is 64.87015913568206, parameters k is 19.237704162289972 and b is -33.49897165223525\n",
      "Iteration 582, the loss is 64.4651928418552, parameters k is 19.174857818416456 and b is -33.508971652235246\n",
      "Iteration 583, the loss is 64.0602265480284, parameters k is 19.11201147454294 and b is -33.518971652235244\n",
      "Iteration 584, the loss is 63.65526025420154, parameters k is 19.049165130669422 and b is -33.52897165223524\n",
      "Iteration 585, the loss is 63.250293960374734, parameters k is 18.986318786795906 and b is -33.53897165223524\n",
      "Iteration 586, the loss is 62.84532766654793, parameters k is 18.92347244292239 and b is -33.54897165223524\n",
      "Iteration 587, the loss is 62.44036137272108, parameters k is 18.860626099048872 and b is -33.558971652235236\n",
      "Iteration 588, the loss is 62.03539507889422, parameters k is 18.797779755175355 and b is -33.568971652235234\n",
      "Iteration 589, the loss is 61.630428785067394, parameters k is 18.73493341130184 and b is -33.57897165223523\n",
      "Iteration 590, the loss is 61.22546249124054, parameters k is 18.672087067428322 and b is -33.58897165223523\n",
      "Iteration 591, the loss is 60.82049619741372, parameters k is 18.609240723554805 and b is -33.59897165223523\n",
      "Iteration 592, the loss is 60.41552990358696, parameters k is 18.54639437968129 and b is -33.608971652235226\n",
      "Iteration 593, the loss is 60.0105636097601, parameters k is 18.48354803580777 and b is -33.618971652235224\n",
      "Iteration 594, the loss is 59.60559731593321, parameters k is 18.420701691934255 and b is -33.62897165223522\n",
      "Iteration 595, the loss is 59.20063102210637, parameters k is 18.35785534806074 and b is -33.63897165223522\n",
      "Iteration 596, the loss is 58.79566472827952, parameters k is 18.29500900418722 and b is -33.64897165223522\n",
      "Iteration 597, the loss is 58.39069843445271, parameters k is 18.232162660313705 and b is -33.658971652235216\n",
      "Iteration 598, the loss is 57.985732140625885, parameters k is 18.169316316440188 and b is -33.668971652235214\n",
      "Iteration 599, the loss is 57.58076584679904, parameters k is 18.10646997256667 and b is -33.67897165223521\n",
      "Iteration 600, the loss is 57.17579955297221, parameters k is 18.043623628693155 and b is -33.68897165223521\n",
      "Iteration 601, the loss is 56.77083325914533, parameters k is 17.980777284819638 and b is -33.69897165223521\n",
      "Iteration 602, the loss is 56.365866965318496, parameters k is 17.91793094094612 and b is -33.708971652235206\n",
      "Iteration 603, the loss is 55.960900671491665, parameters k is 17.855084597072604 and b is -33.718971652235204\n",
      "Iteration 604, the loss is 55.55593437766483, parameters k is 17.792238253199088 and b is -33.7289716522352\n",
      "Iteration 605, the loss is 55.15096808383801, parameters k is 17.72939190932557 and b is -33.7389716522352\n",
      "Iteration 606, the loss is 54.74600179001121, parameters k is 17.666545565452054 and b is -33.7489716522352\n",
      "Iteration 607, the loss is 54.341035496184325, parameters k is 17.603699221578538 and b is -33.758971652235196\n",
      "Iteration 608, the loss is 53.9360692023575, parameters k is 17.54085287770502 and b is -33.768971652235194\n",
      "Iteration 609, the loss is 53.531102908530684, parameters k is 17.478006533831504 and b is -33.77897165223519\n",
      "Iteration 610, the loss is 53.126136614703874, parameters k is 17.415160189957987 and b is -33.78897165223519\n",
      "Iteration 611, the loss is 52.72117032087704, parameters k is 17.35231384608447 and b is -33.79897165223519\n",
      "Iteration 612, the loss is 52.31620402705015, parameters k is 17.289467502210954 and b is -33.808971652235186\n",
      "Iteration 613, the loss is 51.911237733223345, parameters k is 17.226621158337437 and b is -33.818971652235184\n",
      "Iteration 614, the loss is 51.507096615435714, parameters k is 17.16377481446392 and b is -33.82897165223518\n",
      "Iteration 615, the loss is 51.103976372680464, parameters k is 17.101069221578545 and b is -33.83893212654348\n",
      "Iteration 616, the loss is 50.70085612992518, parameters k is 17.03836362869317 and b is -33.848892600851784\n",
      "Iteration 617, the loss is 50.29773588716986, parameters k is 16.975658035807793 and b is -33.858853075160084\n",
      "Iteration 618, the loss is 49.89461564441464, parameters k is 16.912952442922418 and b is -33.868813549468385\n",
      "Iteration 619, the loss is 49.49202127983756, parameters k is 16.850246850037042 and b is -33.878774023776685\n",
      "Iteration 620, the loss is 49.09143937184929, parameters k is 16.787737699839415 and b is -33.88869497239329\n",
      "Iteration 621, the loss is 48.690857463861015, parameters k is 16.725228549641788 and b is -33.89861592100989\n",
      "Iteration 622, the loss is 48.29027555587271, parameters k is 16.66271939944416 and b is -33.908536869626495\n",
      "Iteration 623, the loss is 47.8896936478844, parameters k is 16.600210249246533 and b is -33.9184578182431\n",
      "Iteration 624, the loss is 47.48911173989612, parameters k is 16.537701099048906 and b is -33.9283787668597\n",
      "Iteration 625, the loss is 47.08852983190789, parameters k is 16.47519194885128 and b is -33.938299715476305\n",
      "Iteration 626, the loss is 46.68794792391961, parameters k is 16.412682798653652 and b is -33.94822066409291\n",
      "Iteration 627, the loss is 46.28736601593129, parameters k is 16.350173648456025 and b is -33.95814161270951\n",
      "Iteration 628, the loss is 45.88678410794296, parameters k is 16.287664498258398 and b is -33.968062561326114\n",
      "Iteration 629, the loss is 45.48620219995472, parameters k is 16.22515534806077 and b is -33.97798350994272\n",
      "Iteration 630, the loss is 45.085620291966414, parameters k is 16.162646197863143 and b is -33.98790445855932\n",
      "Iteration 631, the loss is 44.6850383839782, parameters k is 16.100137047665516 and b is -33.997825407175924\n",
      "Iteration 632, the loss is 44.28445647598987, parameters k is 16.03762789746789 and b is -34.00774635579253\n",
      "Iteration 633, the loss is 43.8838745680016, parameters k is 15.97511874727026 and b is -34.01766730440913\n",
      "Iteration 634, the loss is 43.483292660013305, parameters k is 15.912609597072631 and b is -34.02758825302573\n",
      "Iteration 635, the loss is 43.08271075202503, parameters k is 15.850100446875002 and b is -34.03750920164234\n",
      "Iteration 636, the loss is 42.68212884403665, parameters k is 15.787591296677373 and b is -34.04743015025894\n",
      "Iteration 637, the loss is 42.2815469360484, parameters k is 15.725082146479744 and b is -34.05735109887554\n",
      "Iteration 638, the loss is 41.88096502806006, parameters k is 15.662572996282115 and b is -34.067272047492146\n",
      "Iteration 639, the loss is 41.48038312007184, parameters k is 15.600063846084486 and b is -34.07719299610875\n",
      "Iteration 640, the loss is 41.07980121208355, parameters k is 15.537554695886858 and b is -34.08711394472535\n",
      "Iteration 641, the loss is 40.679219304095184, parameters k is 15.475045545689229 and b is -34.097034893341956\n",
      "Iteration 642, the loss is 40.278637396106866, parameters k is 15.4125363954916 and b is -34.10695584195856\n",
      "Iteration 643, the loss is 39.878055488118626, parameters k is 15.35002724529397 and b is -34.11687679057516\n",
      "Iteration 644, the loss is 39.47747358013036, parameters k is 15.287518095096342 and b is -34.126797739191765\n",
      "Iteration 645, the loss is 39.07689167214199, parameters k is 15.225008944898713 and b is -34.13671868780837\n",
      "Iteration 646, the loss is 38.67630976415369, parameters k is 15.162499794701084 and b is -34.14663963642497\n",
      "Iteration 647, the loss is 38.27572785616543, parameters k is 15.099990644503455 and b is -34.156560585041575\n",
      "Iteration 648, the loss is 37.87514594817712, parameters k is 15.037481494305826 and b is -34.16648153365818\n",
      "Iteration 649, the loss is 37.474564040188845, parameters k is 14.974972344108197 and b is -34.17640248227478\n",
      "Iteration 650, the loss is 37.07398213220053, parameters k is 14.912463193910568 and b is -34.186323430891385\n",
      "Iteration 651, the loss is 36.673400224212294, parameters k is 14.84995404371294 and b is -34.19624437950799\n",
      "Iteration 652, the loss is 36.27353873402826, parameters k is 14.78744489351531 and b is -34.20616532812459\n",
      "Iteration 653, the loss is 35.87494164119095, parameters k is 14.725088431064718 and b is -34.21604675104949\n",
      "Iteration 654, the loss is 35.476344548353644, parameters k is 14.662731968614125 and b is -34.22592817397439\n",
      "Iteration 655, the loss is 35.07774745551634, parameters k is 14.600375506163532 and b is -34.23580959689929\n",
      "Iteration 656, the loss is 34.67915036267903, parameters k is 14.53801904371294 and b is -34.245691019824186\n",
      "Iteration 657, the loss is 34.28055326984177, parameters k is 14.475662581262347 and b is -34.255572442749084\n",
      "Iteration 658, the loss is 33.88195617700445, parameters k is 14.413306118811754 and b is -34.26545386567398\n",
      "Iteration 659, the loss is 33.483359084167155, parameters k is 14.350949656361161 and b is -34.27533528859888\n",
      "Iteration 660, the loss is 33.08610480436268, parameters k is 14.288593193910568 and b is -34.28521671152378\n",
      "Iteration 661, the loss is 32.69047627871907, parameters k is 14.226468944898711 and b is -34.29505860875698\n",
      "Iteration 662, the loss is 32.29484775307544, parameters k is 14.164344695886854 and b is -34.30490050599018\n",
      "Iteration 663, the loss is 31.89921922743186, parameters k is 14.102220446874997 and b is -34.314742403223384\n",
      "Iteration 664, the loss is 31.50359070178822, parameters k is 14.04009619786314 and b is -34.324584300456586\n",
      "Iteration 665, the loss is 31.107962176144575, parameters k is 13.977971948851282 and b is -34.33442619768979\n",
      "Iteration 666, the loss is 30.71233365050098, parameters k is 13.915847699839425 and b is -34.34426809492299\n",
      "Iteration 667, the loss is 30.316705124857364, parameters k is 13.853723450827568 and b is -34.35410999215619\n",
      "Iteration 668, the loss is 29.921076599213738, parameters k is 13.79159920181571 and b is -34.36395188938939\n",
      "Iteration 669, the loss is 29.525448073570104, parameters k is 13.729474952803853 and b is -34.37379378662259\n",
      "Iteration 670, the loss is 29.129819547926505, parameters k is 13.667350703791996 and b is -34.38363568385579\n",
      "Iteration 671, the loss is 28.734191022282875, parameters k is 13.605226454780139 and b is -34.393477581088995\n",
      "Iteration 672, the loss is 28.33942967516601, parameters k is 13.543102205768282 and b is -34.403319478322196\n",
      "Iteration 673, the loss is 27.94692544087767, parameters k is 13.481223648456028 and b is -34.4131218498637\n",
      "Iteration 674, the loss is 27.554421206589303, parameters k is 13.419345091143775 and b is -34.422924221405204\n",
      "Iteration 675, the loss is 27.161916972300993, parameters k is 13.357466533831522 and b is -34.43272659294671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 676, the loss is 26.769412738012615, parameters k is 13.295587976519268 and b is -34.44252896448821\n",
      "Iteration 677, the loss is 26.376908503724245, parameters k is 13.233709419207015 and b is -34.452331336029715\n",
      "Iteration 678, the loss is 25.984404269435924, parameters k is 13.171830861894762 and b is -34.46213370757122\n",
      "Iteration 679, the loss is 25.591900035147574, parameters k is 13.109952304582508 and b is -34.47193607911272\n",
      "Iteration 680, the loss is 25.19939580085921, parameters k is 13.048073747270255 and b is -34.48173845065423\n",
      "Iteration 681, the loss is 24.80689156657088, parameters k is 12.986195189958002 and b is -34.49154082219573\n",
      "Iteration 682, the loss is 24.414387332282534, parameters k is 12.924316632645748 and b is -34.501343193737235\n",
      "Iteration 683, the loss is 24.02188309799419, parameters k is 12.862438075333495 and b is -34.51114556527874\n",
      "Iteration 684, the loss is 23.629378863705874, parameters k is 12.800559518021242 and b is -34.52094793682024\n",
      "Iteration 685, the loss is 23.236874629417514, parameters k is 12.738680960708988 and b is -34.530750308361746\n",
      "Iteration 686, the loss is 22.844370395129157, parameters k is 12.676802403396735 and b is -34.54055267990325\n",
      "Iteration 687, the loss is 22.45283382482523, parameters k is 12.614923846084482 and b is -34.550355051444754\n",
      "Iteration 688, the loss is 22.063668992507868, parameters k is 12.553309438969857 and b is -34.56011789729455\n",
      "Iteration 689, the loss is 21.67450416019051, parameters k is 12.491695031855231 and b is -34.56988074314435\n",
      "Iteration 690, the loss is 21.28533932787314, parameters k is 12.430080624740606 and b is -34.57964358899415\n",
      "Iteration 691, the loss is 20.896174495555748, parameters k is 12.368466217625981 and b is -34.58940643484395\n",
      "Iteration 692, the loss is 20.50700966323839, parameters k is 12.306851810511356 and b is -34.59916928069375\n",
      "Iteration 693, the loss is 20.11784483092101, parameters k is 12.24523740339673 and b is -34.60893212654355\n",
      "Iteration 694, the loss is 19.728679998603635, parameters k is 12.183622996282105 and b is -34.61869497239335\n",
      "Iteration 695, the loss is 19.339515166286304, parameters k is 12.12200858916748 and b is -34.62845781824315\n",
      "Iteration 696, the loss is 18.950439247339702, parameters k is 12.060394182052855 and b is -34.63822066409295\n",
      "Iteration 697, the loss is 18.56476103124316, parameters k is 11.999057087191195 and b is -34.64794398425105\n",
      "Iteration 698, the loss is 18.179082815146632, parameters k is 11.937719992329535 and b is -34.65766730440915\n",
      "Iteration 699, the loss is 17.793404599050124, parameters k is 11.876382897467876 and b is -34.667390624567254\n",
      "Iteration 700, the loss is 17.407726382953598, parameters k is 11.815045802606216 and b is -34.677113944725356\n",
      "Iteration 701, the loss is 17.02204816685709, parameters k is 11.753708707744556 and b is -34.68683726488346\n",
      "Iteration 702, the loss is 16.63681867349152, parameters k is 11.692371612882896 and b is -34.69656058504156\n",
      "Iteration 703, the loss is 16.25322091633388, parameters k is 11.631198075333488 and b is -34.706244379507964\n",
      "Iteration 704, the loss is 15.86962315917625, parameters k is 11.57002453778408 and b is -34.71592817397437\n",
      "Iteration 705, the loss is 15.486876119105567, parameters k is 11.508851000234673 and b is -34.725611968440774\n",
      "Iteration 706, the loss is 15.106320007986467, parameters k is 11.447920308535068 and b is -34.735256237215474\n",
      "Iteration 707, the loss is 14.726282754723652, parameters k is 11.386989616835463 and b is -34.744900505990174\n",
      "Iteration 708, the loss is 14.349124549426792, parameters k is 11.326255091143762 and b is -34.754505249073176\n",
      "Iteration 709, the loss is 13.975071845778835, parameters k is 11.265703490353248 and b is -34.76407046646448\n",
      "Iteration 710, the loss is 13.604967837293733, parameters k is 11.205611454780126 and b is -34.773556632472385\n",
      "Iteration 711, the loss is 13.235571219700232, parameters k is 11.145519419207004 and b is -34.78304279848029\n",
      "Iteration 712, the loss is 12.86820113219372, parameters k is 11.085649043712932 and b is -34.792489438796494\n",
      "Iteration 713, the loss is 12.500831044687239, parameters k is 11.02577866821886 and b is -34.8019360791127\n",
      "Iteration 714, the loss is 12.133460957180757, parameters k is 10.965908292724789 and b is -34.811382719428906\n",
      "Iteration 715, the loss is 11.768081387350145, parameters k is 10.906037917230718 and b is -34.82082935974511\n",
      "Iteration 716, the loss is 11.407582136969951, parameters k is 10.846663470590402 and b is -34.830196948677916\n",
      "Iteration 717, the loss is 11.052051809912388, parameters k is 10.787597403396726 and b is -34.83952501191902\n",
      "Iteration 718, the loss is 10.702186368891217, parameters k is 10.729054774938227 and b is -34.84877402377673\n",
      "Iteration 719, the loss is 10.35683737181691, parameters k is 10.670823411301864 and b is -34.857983509942734\n",
      "Iteration 720, the loss is 10.016754574941528, parameters k is 10.613218608930323 and b is -34.86711394472534\n",
      "Iteration 721, the loss is 9.68371054512811, parameters k is 10.555924201815698 and b is -34.87620485381625\n",
      "Iteration 722, the loss is 9.361276294605485, parameters k is 10.499847936993563 and b is -34.88513766014036\n",
      "Iteration 723, the loss is 9.041263543857315, parameters k is 10.443771672171428 and b is -34.89407046646447\n",
      "Iteration 724, the loss is 8.725457716130709, parameters k is 10.388233865847317 and b is -34.90292422140518\n",
      "Iteration 725, the loss is 8.416672987429461, parameters k is 10.33312823343625 and b is -34.911698924962494\n",
      "Iteration 726, the loss is 8.115663644506018, parameters k is 10.278814340155618 and b is -34.92035505144471\n",
      "Iteration 727, the loss is 7.824344399130907, parameters k is 10.225045585214907 and b is -34.92893212654352\n",
      "Iteration 728, the loss is 7.544683867155516, parameters k is 10.172805170195144 and b is -34.93727204749214\n",
      "Iteration 729, the loss is 7.271377339255078, parameters k is 10.120847561499492 and b is -34.945572442749054\n",
      "Iteration 730, the loss is 7.012277404201721, parameters k is 10.070198767033089 and b is -34.95367520954747\n",
      "Iteration 731, the loss is 6.768207667058595, parameters k is 10.021172225531112 and b is -34.961540822195694\n",
      "Iteration 732, the loss is 6.5358505170423165, parameters k is 9.973230269009372 and b is -34.96920880638542\n",
      "Iteration 733, the loss is 6.319398129112349, parameters k is 9.926536691934274 and b is -34.97667916211664\n",
      "Iteration 734, the loss is 6.123039252525644, parameters k is 9.882336355965894 and b is -34.983754260930866\n",
      "Iteration 735, the loss is 5.943017169390491, parameters k is 9.839899340155617 and b is -34.990552679903196\n",
      "Iteration 736, the loss is 5.776598047560028, parameters k is 9.799099794701071 and b is -34.99707441903363\n",
      "Iteration 737, the loss is 5.621023256679632, parameters k is 9.759579300629925 and b is -35.00339852970557\n",
      "Iteration 738, the loss is 5.482899471149858, parameters k is 9.722594340155617 and b is -35.00932738346051\n",
      "Iteration 739, the loss is 5.357651754534614, parameters k is 9.68675127691451 and b is -35.01509813444865\n",
      "Iteration 740, the loss is 5.2478120164843265, parameters k is 9.653686988376961 and b is -35.0204341028281\n",
      "Iteration 741, the loss is 5.14736962701769, parameters k is 9.621988628693167 and b is -35.025572442749045\n",
      "Iteration 742, the loss is 5.0567958734500795, parameters k is 9.59205090142044 and b is -35.030434102828096\n",
      "Iteration 743, the loss is 4.973395903951008, parameters k is 9.563106929088423 and b is -35.03513766014035\n",
      "Iteration 744, the loss is 4.900227738321413, parameters k is 9.535955051618068 and b is -35.03956453761071\n",
      "Iteration 745, the loss is 4.836124941440407, parameters k is 9.510424004187238 and b is -35.04371473523916\n",
      "Iteration 746, the loss is 4.78194313553813, parameters k is 9.4871329962821 and b is -35.04750920164233\n",
      "Iteration 747, the loss is 4.735615917278949, parameters k is 9.465602857942178 and b is -35.051026988203596\n",
      "Iteration 748, the loss is 4.692633068079113, parameters k is 9.445030980471822 and b is -35.05438667199806\n",
      "Iteration 749, the loss is 4.653158952378904, parameters k is 9.424965427112138 and b is -35.05766730440913\n",
      "Iteration 750, the loss is 4.622674766118993, parameters k is 9.40734860893032 and b is -35.0605526799032\n",
      "Iteration 751, the loss is 4.596432709033752, parameters k is 9.391118786795932 and b is -35.06320090124708\n",
      "Iteration 752, the loss is 4.573627276026165, parameters k is 9.375893826321624 and b is -35.06569101982415\n",
      "Iteration 753, the loss is 4.555443898425848, parameters k is 9.362413767033086 and b is -35.067904458559326\n",
      "Iteration 754, the loss is 4.5401919669000455, parameters k is 9.34993236387103 and b is -35.069959794527705\n",
      "Iteration 755, the loss is 4.526921673413886, parameters k is 9.338456612882888 and b is -35.07185702772929\n",
      "Iteration 756, the loss is 4.514506286829292, parameters k is 9.32746074331767 and b is -35.07367520954747\n",
      "Iteration 757, the loss is 4.5032388473540585, parameters k is 9.316706296677355 and b is -35.07545386567395\n",
      "Iteration 758, the loss is 4.493634776646769, parameters k is 9.306903628693165 and b is -35.077074419033636\n",
      "Iteration 759, the loss is 4.486361548084898, parameters k is 9.298033332250478 and b is -35.07853686962652\n",
      "Iteration 760, the loss is 4.481402833879168, parameters k is 9.290889122764312 and b is -35.079722640377504\n",
      "Iteration 761, the loss is 4.476933137971988, parameters k is 9.284295782843364 and b is -35.080829359745096\n",
      "Iteration 762, the loss is 4.472557433473232, parameters k is 9.277702442922415 and b is -35.08193607911269\n",
      "Iteration 763, the loss is 4.468781358110086, parameters k is 9.27155617810028 and b is -35.082963747096876\n",
      "Iteration 764, the loss is 4.465635832657694, parameters k is 9.265885842131901 and b is -35.083912363697664\n",
      "Iteration 765, the loss is 4.463151772433383, parameters k is 9.260934755175379 and b is -35.08474240322335\n",
      "Iteration 766, the loss is 4.460883271117905, parameters k is 9.25623793699356 and b is -35.08553291705734\n",
      "Iteration 767, the loss is 4.4586147698024226, parameters k is 9.251541118811742 and b is -35.08632343089133\n",
      "Iteration 768, the loss is 4.4563462684869455, parameters k is 9.246844300629924 and b is -35.087113944725324\n",
      "Iteration 769, the loss is 4.454107322788111, parameters k is 9.242147482448106 and b is -35.087904458559315\n",
      "Iteration 770, the loss is 4.452185257396885, parameters k is 9.237692640550872 and b is -35.08865544670161\n",
      "Iteration 771, the loss is 4.45059522913176, parameters k is 9.233762146479725 and b is -35.0893273834605\n",
      "Iteration 772, the loss is 4.449105016488487, parameters k is 9.229831652408578 and b is -35.08999932021939\n",
      "Iteration 773, the loss is 4.4480137311126136, parameters k is 9.22644696861411 and b is -35.09059220559489\n",
      "Iteration 774, the loss is 4.447227309200241, parameters k is 9.223553905373004 and b is -35.09110603958698\n",
      "Iteration 775, the loss is 4.446623119468133, parameters k is 9.2211346365983 and b is -35.091540822195675\n",
      "Iteration 776, the loss is 4.446070716252604, parameters k is 9.218715367823597 and b is -35.09197560480437\n",
      "Iteration 777, the loss is 4.4455694659968215, parameters k is 9.216511672171423 and b is -35.09237086172136\n",
      "Iteration 778, the loss is 4.4451381798493035, parameters k is 9.21430797651925 and b is -35.09276611863836\n",
      "Iteration 779, the loss is 4.444851615307989, parameters k is 9.212565703791977 and b is -35.09308232417195\n",
      "Iteration 780, the loss is 4.4446221076278505, parameters k is 9.21106706742834 and b is -35.09335900401385\n",
      "Iteration 781, the loss is 4.444479801442426, parameters k is 9.209897956756404 and b is -35.09359615816405\n",
      "Iteration 782, the loss is 4.44434500588844, parameters k is 9.208728846084467 and b is -35.093833312314246\n",
      "Iteration 783, the loss is 4.444264364665138, parameters k is 9.207852857942175 and b is -35.09403094077275\n",
      "Iteration 784, the loss is 4.44418372344183, parameters k is 9.206976869799883 and b is -35.09422856923125\n",
      "Iteration 785, the loss is 4.444103082218527, parameters k is 9.20610088165759 and b is -35.09442619768975\n",
      "Iteration 786, the loss is 4.444022440995225, parameters k is 9.205224893515298 and b is -35.09462382614825\n",
      "Iteration 787, the loss is 4.443941799771918, parameters k is 9.204348905373006 and b is -35.09482145460675\n",
      "Iteration 788, the loss is 4.443871822696855, parameters k is 9.203472917230714 and b is -35.09501908306525\n",
      "Iteration 789, the loss is 4.443830072575741, parameters k is 9.20284641525443 and b is -35.09517718583205\n",
      "Iteration 790, the loss is 4.4437883224546235, parameters k is 9.202219913278144 and b is -35.09533528859885\n",
      "Iteration 791, the loss is 4.443746572333509, parameters k is 9.20159341130186 and b is -35.09549339136564\n",
      "Iteration 792, the loss is 4.443722133159254, parameters k is 9.200966909325574 and b is -35.09565149413244\n",
      "Iteration 793, the loss is 4.443708783662735, parameters k is 9.200621316440198 and b is -35.09577007120754\n",
      "Iteration 794, the loss is 4.443695434166218, parameters k is 9.200275723554823 and b is -35.09588864828264\n",
      "Iteration 795, the loss is 4.443682084669705, parameters k is 9.199930130669447 and b is -35.09600722535774\n",
      "Iteration 796, the loss is 4.443668735173186, parameters k is 9.199584537784071 and b is -35.09612580243284\n",
      "Iteration 797, the loss is 4.4436553856766725, parameters k is 9.199238944898696 and b is -35.09624437950794\n",
      "Iteration 798, the loss is 4.443642036180158, parameters k is 9.19889335201332 and b is -35.096362956583036\n",
      "Iteration 799, the loss is 4.443628686683642, parameters k is 9.198547759127944 and b is -35.096481533658135\n",
      "Iteration 800, the loss is 4.443615337187124, parameters k is 9.198202166242568 and b is -35.096600110733235\n",
      "Iteration 801, the loss is 4.443601987690604, parameters k is 9.197856573357193 and b is -35.096718687808334\n",
      "Iteration 802, the loss is 4.443588638194093, parameters k is 9.197510980471817 and b is -35.09683726488343\n",
      "Iteration 803, the loss is 4.443575288697576, parameters k is 9.197165387586441 and b is -35.09695584195853\n",
      "Iteration 804, the loss is 4.443569075452083, parameters k is 9.196819794701065 and b is -35.09707441903363\n",
      "Iteration 805, the loss is 4.443567587403491, parameters k is 9.196726889562726 and b is -35.097153470417034\n",
      "Iteration 806, the loss is 4.443566099354896, parameters k is 9.196633984424386 and b is -35.097232521800436\n",
      "Iteration 807, the loss is 4.443564611306299, parameters k is 9.196541079286046 and b is -35.09731157318384\n",
      "Iteration 808, the loss is 4.443563123257705, parameters k is 9.196448174147706 and b is -35.09739062456724\n",
      "Iteration 809, the loss is 4.44356163520911, parameters k is 9.196355269009366 and b is -35.09746967595064\n",
      "Iteration 810, the loss is 4.443560147160518, parameters k is 9.196262363871027 and b is -35.09754872733404\n",
      "Iteration 811, the loss is 4.443558659111922, parameters k is 9.196169458732687 and b is -35.097627778717445\n",
      "Iteration 812, the loss is 4.443557171063329, parameters k is 9.196076553594347 and b is -35.09770683010085\n",
      "Iteration 813, the loss is 4.4435556830147345, parameters k is 9.195983648456007 and b is -35.09778588148425\n",
      "Iteration 814, the loss is 4.443554194966136, parameters k is 9.195890743317667 and b is -35.09786493286765\n",
      "Iteration 815, the loss is 4.4435527069175444, parameters k is 9.195797838179327 and b is -35.09794398425105\n",
      "Iteration 816, the loss is 4.44355121886895, parameters k is 9.195704933040988 and b is -35.098023035634455\n",
      "Iteration 817, the loss is 4.443549730820355, parameters k is 9.195612027902648 and b is -35.09810208701786\n",
      "Iteration 818, the loss is 4.443548242771756, parameters k is 9.195519122764308 and b is -35.09818113840126\n",
      "Iteration 819, the loss is 4.443546754723164, parameters k is 9.195426217625968 and b is -35.09826018978466\n",
      "Iteration 820, the loss is 4.44354526667457, parameters k is 9.195333312487628 and b is -35.09833924116806\n",
      "Iteration 821, the loss is 4.443543778625975, parameters k is 9.195240407349289 and b is -35.098418292551465\n",
      "Iteration 822, the loss is 4.443542290577379, parameters k is 9.195147502210949 and b is -35.098497343934866\n",
      "Iteration 823, the loss is 4.443540802528783, parameters k is 9.195054597072609 and b is -35.09857639531827\n",
      "Iteration 824, the loss is 4.443539314480193, parameters k is 9.19496169193427 and b is -35.09865544670167\n",
      "Iteration 825, the loss is 4.443537826431598, parameters k is 9.19486878679593 and b is -35.09873449808507\n",
      "Iteration 826, the loss is 4.443536338382998, parameters k is 9.19477588165759 and b is -35.098813549468474\n",
      "Iteration 827, the loss is 4.443534850334409, parameters k is 9.19468297651925 and b is -35.098892600851876\n",
      "Iteration 828, the loss is 4.443533362285809, parameters k is 9.19459007138091 and b is -35.09897165223528\n",
      "Iteration 829, the loss is 4.44353187423722, parameters k is 9.19449716624257 and b is -35.09905070361868\n",
      "Iteration 830, the loss is 4.443530386188621, parameters k is 9.19440426110423 and b is -35.09912975500208\n",
      "Iteration 831, the loss is 4.4435288981400305, parameters k is 9.19431135596589 and b is -35.099208806385484\n",
      "Iteration 832, the loss is 4.4435274100914315, parameters k is 9.19421845082755 and b is -35.099287857768886\n",
      "Iteration 833, the loss is 4.443525922042841, parameters k is 9.19412554568921 and b is -35.09936690915229\n",
      "Iteration 834, the loss is 4.443524433994239, parameters k is 9.194032640550871 and b is -35.09944596053569\n",
      "Iteration 835, the loss is 4.4435229459456504, parameters k is 9.193939735412531 and b is -35.09952501191909\n",
      "Iteration 836, the loss is 4.443521457897055, parameters k is 9.193846830274191 and b is -35.09960406330249\n",
      "Iteration 837, the loss is 4.443519969848459, parameters k is 9.193753925135852 and b is -35.099683114685895\n",
      "Iteration 838, the loss is 4.4435184817998685, parameters k is 9.193661019997512 and b is -35.0997621660693\n",
      "Iteration 839, the loss is 4.443516993751268, parameters k is 9.193568114859172 and b is -35.0998412174527\n",
      "Iteration 840, the loss is 4.443515505702672, parameters k is 9.193475209720832 and b is -35.0999202688361\n",
      "Iteration 841, the loss is 4.443514017654078, parameters k is 9.193382304582492 and b is -35.0999993202195\n",
      "Iteration 842, the loss is 4.4435125296054885, parameters k is 9.193289399444152 and b is -35.100078371602905\n",
      "Iteration 843, the loss is 4.443511041556891, parameters k is 9.193196494305813 and b is -35.10015742298631\n",
      "Iteration 844, the loss is 4.443509553508293, parameters k is 9.193103589167473 and b is -35.10023647436971\n",
      "Iteration 845, the loss is 4.443508065459703, parameters k is 9.193010684029133 and b is -35.10031552575311\n",
      "Iteration 846, the loss is 4.443508576442077, parameters k is 9.192917778890793 and b is -35.10039457713651\n",
      "Iteration 847, the loss is 4.443507626768107, parameters k is 9.193064359918461 and b is -35.10043410282821\n",
      "Iteration 848, the loss is 4.443507320971293, parameters k is 9.192971454780121 and b is -35.10051315421161\n",
      "Iteration 849, the loss is 4.443507188076522, parameters k is 9.193118035807789 and b is -35.10055267990331\n",
      "Iteration 850, the loss is 4.443506065500497, parameters k is 9.193025130669449 and b is -35.10063173128671\n",
      "Iteration 851, the loss is 4.443506749384927, parameters k is 9.193171711697117 and b is -35.10067125697841\n",
      "Iteration 852, the loss is 4.44350526133634, parameters k is 9.193078806558777 and b is -35.10075030836181\n",
      "Iteration 853, the loss is 4.443505859386707, parameters k is 9.192985901420437 and b is -35.10082935974521\n",
      "Iteration 854, the loss is 4.44350482264474, parameters k is 9.193132482448105 and b is -35.10086888543691\n",
      "Iteration 855, the loss is 4.443504603915914, parameters k is 9.193039577309765 and b is -35.10094793682031\n",
      "Iteration 856, the loss is 4.443504383953158, parameters k is 9.193186158337433 and b is -35.10098746251201\n",
      "Iteration 857, the loss is 4.4435033484451205, parameters k is 9.193093253199093 and b is -35.10106651389541\n",
      "Iteration 858, the loss is 4.44350394526156, parameters k is 9.19323983422676 and b is -35.10110603958711\n",
      "Iteration 859, the loss is 4.443502457212974, parameters k is 9.193146929088421 and b is -35.10118509097051\n",
      "Iteration 860, the loss is 4.443503142331333, parameters k is 9.193054023950081 and b is -35.10126414235391\n",
      "Iteration 861, the loss is 4.443502018521379, parameters k is 9.193200604977749 and b is -35.10130366804561\n",
      "Iteration 862, the loss is 4.443501886860537, parameters k is 9.193107699839409 and b is -35.10138271942901\n",
      "Iteration 863, the loss is 4.443501579829791, parameters k is 9.193254280867077 and b is -35.10142224512071\n",
      "Iteration 864, the loss is 4.443500631389745, parameters k is 9.193161375728737 and b is -35.10150129650411\n",
      "Iteration 865, the loss is 4.443501141138195, parameters k is 9.193307956756405 and b is -35.10154082219581\n",
      "Iteration 866, the loss is 4.443499653089609, parameters k is 9.193215051618065 and b is -35.10161987357921\n",
      "Iteration 867, the loss is 4.44350042527596, parameters k is 9.193122146479725 and b is -35.10169892496261\n",
      "Iteration 868, the loss is 4.443499214398011, parameters k is 9.193268727507393 and b is -35.10173845065431\n",
      "Iteration 869, the loss is 4.443499169805166, parameters k is 9.193175822369053 and b is -35.10181750203771\n",
      "Iteration 870, the loss is 4.443498775706423, parameters k is 9.19332240339672 and b is -35.10185702772941\n",
      "Iteration 871, the loss is 4.443497914334369, parameters k is 9.19322949825838 and b is -35.10193607911281\n",
      "Iteration 872, the loss is 4.443498337014834, parameters k is 9.193376079286049 and b is -35.10197560480451\n",
      "Iteration 873, the loss is 4.443496848966242, parameters k is 9.193283174147709 and b is -35.10205465618791\n",
      "Iteration 874, the loss is 4.4434977082205815, parameters k is 9.193190269009369 and b is -35.10213370757131\n",
      "Iteration 875, the loss is 4.443496410274651, parameters k is 9.193336850037037 and b is -35.10217323326301\n",
      "Iteration 876, the loss is 4.443496452749787, parameters k is 9.193243944898697 and b is -35.10225228464641\n",
      "Iteration 877, the loss is 4.443495971583059, parameters k is 9.193390525926365 and b is -35.10229181033811\n",
      "Iteration 878, the loss is 4.443495197278997, parameters k is 9.193297620788025 and b is -35.10237086172151\n",
      "Iteration 879, the loss is 4.443495532891468, parameters k is 9.193444201815693 and b is -35.10241038741321\n",
      "Iteration 880, the loss is 4.443494044842875, parameters k is 9.193351296677353 and b is -35.10248943879661\n",
      "Iteration 881, the loss is 4.443494991165206, parameters k is 9.193258391539013 and b is -35.10256849018001\n",
      "Iteration 882, the loss is 4.443493606151283, parameters k is 9.19340497256668 and b is -35.10260801587171\n",
      "Iteration 883, the loss is 4.443493735694419, parameters k is 9.19331206742834 and b is -35.10268706725511\n",
      "Iteration 884, the loss is 4.443493167459695, parameters k is 9.193458648456009 and b is -35.10272659294681\n",
      "Iteration 885, the loss is 4.443492480223623, parameters k is 9.193365743317669 and b is -35.10280564433021\n",
      "Iteration 886, the loss is 4.443492728768105, parameters k is 9.193512324345336 and b is -35.10284517002191\n",
      "Iteration 887, the loss is 4.443491240719506, parameters k is 9.193419419206997 and b is -35.10292422140531\n",
      "Iteration 888, the loss is 4.443492274109833, parameters k is 9.193326514068657 and b is -35.10300327278871\n",
      "Iteration 889, the loss is 4.443490802027916, parameters k is 9.193473095096325 and b is -35.10304279848041\n",
      "Iteration 890, the loss is 4.443491018639041, parameters k is 9.193380189957985 and b is -35.10312184986381\n",
      "Iteration 891, the loss is 4.443490363336326, parameters k is 9.193526770985653 and b is -35.10316137555551\n",
      "Iteration 892, the loss is 4.4434897631682455, parameters k is 9.193433865847313 and b is -35.10324042693891\n",
      "Iteration 893, the loss is 4.443489924644737, parameters k is 9.19358044687498 and b is -35.10327995263061\n",
      "Iteration 894, the loss is 4.443488507697458, parameters k is 9.19348754173664 and b is -35.10335900401401\n",
      "Iteration 895, the loss is 4.443489485953143, parameters k is 9.193634122764308 and b is -35.10339852970571\n",
      "Iteration 896, the loss is 4.443487997904549, parameters k is 9.193541217625969 and b is -35.10347758108911\n",
      "Iteration 897, the loss is 4.443488301583664, parameters k is 9.193448312487629 and b is -35.10355663247251\n",
      "Iteration 898, the loss is 4.4434875592129615, parameters k is 9.193594893515296 and b is -35.10359615816421\n",
      "Iteration 899, the loss is 4.443487046112868, parameters k is 9.193501988376957 and b is -35.10367520954761\n",
      "Iteration 900, the loss is 4.4434871205213735, parameters k is 9.193648569404624 and b is -35.10371473523931\n",
      "Iteration 901, the loss is 4.443485790642081, parameters k is 9.193555664266285 and b is -35.10379378662271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 902, the loss is 4.443486681829786, parameters k is 9.193702245293952 and b is -35.10383331231441\n",
      "Iteration 903, the loss is 4.443485193781181, parameters k is 9.193609340155612 and b is -35.10391236369781\n",
      "Iteration 904, the loss is 4.443485584528291, parameters k is 9.193516435017273 and b is -35.10399141508121\n",
      "Iteration 905, the loss is 4.443484755089596, parameters k is 9.19366301604494 and b is -35.10403094077291\n",
      "Iteration 906, the loss is 4.443484329057499, parameters k is 9.1935701109066 and b is -35.10410999215631\n",
      "Iteration 907, the loss is 4.4434843163980045, parameters k is 9.193716691934268 and b is -35.10414951784801\n",
      "Iteration 908, the loss is 4.443483073586702, parameters k is 9.193623786795929 and b is -35.10422856923141\n",
      "Iteration 909, the loss is 4.44348387770642, parameters k is 9.193770367823596 and b is -35.10426809492311\n",
      "Iteration 910, the loss is 4.44348238965782, parameters k is 9.193677462685256 and b is -35.10434714630651\n",
      "Iteration 911, the loss is 4.443482867472915, parameters k is 9.193584557546917 and b is -35.10442619768991\n",
      "Iteration 912, the loss is 4.443481950966234, parameters k is 9.193731138574584 and b is -35.10446572338161\n",
      "Iteration 913, the loss is 4.44348161200212, parameters k is 9.193638233436245 and b is -35.10454477476501\n",
      "Iteration 914, the loss is 4.44348151227464, parameters k is 9.193784814463912 and b is -35.10458430045671\n",
      "Iteration 915, the loss is 4.44348035653133, parameters k is 9.193691909325572 and b is -35.10466335184011\n",
      "Iteration 916, the loss is 4.443481073583052, parameters k is 9.19383849035324 and b is -35.10470287753181\n",
      "Iteration 917, the loss is 4.443479585534451, parameters k is 9.1937455852149 and b is -35.10478192891521\n",
      "Iteration 918, the loss is 4.443480150417536, parameters k is 9.19365268007656 and b is -35.10486098029861\n",
      "Iteration 919, the loss is 4.443479146842866, parameters k is 9.193799261104228 and b is -35.10490050599031\n",
      "Iteration 920, the loss is 4.443478894946744, parameters k is 9.193706355965888 and b is -35.10497955737371\n",
      "Iteration 921, the loss is 4.443478708151275, parameters k is 9.193852936993556 and b is -35.10501908306541\n",
      "Iteration 922, the loss is 4.443477639475951, parameters k is 9.193760031855216 and b is -35.10509813444881\n",
      "Iteration 923, the loss is 4.4434782694596855, parameters k is 9.193906612882884 and b is -35.10513766014051\n",
      "Iteration 924, the loss is 4.443476781411089, parameters k is 9.193813707744544 and b is -35.10521671152391\n",
      "Iteration 925, the loss is 4.443477433362164, parameters k is 9.193720802606205 and b is -35.10529576290731\n",
      "Iteration 926, the loss is 4.443476342719498, parameters k is 9.193867383633872 and b is -35.10533528859901\n",
      "Iteration 927, the loss is 4.443476177891367, parameters k is 9.193774478495532 and b is -35.10541433998241\n",
      "Iteration 928, the loss is 4.443475904027907, parameters k is 9.1939210595232 and b is -35.10545386567411\n",
      "Iteration 929, the loss is 4.443474922420579, parameters k is 9.19382815438486 and b is -35.10553291705751\n",
      "Iteration 930, the loss is 4.443475465336315, parameters k is 9.193974735412528 and b is -35.10557244274921\n",
      "Iteration 931, the loss is 4.443473977287722, parameters k is 9.193881830274188 and b is -35.10565149413261\n",
      "Iteration 932, the loss is 4.443474716306792, parameters k is 9.193788925135848 and b is -35.10573054551601\n",
      "Iteration 933, the loss is 4.443473538596138, parameters k is 9.193935506163516 and b is -35.10577007120771\n",
      "Iteration 934, the loss is 4.443473460835996, parameters k is 9.193842601025176 and b is -35.10584912259111\n",
      "Iteration 935, the loss is 4.443473099904542, parameters k is 9.193989182052844 and b is -35.10588864828281\n",
      "Iteration 936, the loss is 4.443472205365204, parameters k is 9.193896276914504 and b is -35.10596769966621\n",
      "Iteration 937, the loss is 4.443472661212951, parameters k is 9.194042857942172 and b is -35.10600722535791\n",
      "Iteration 938, the loss is 4.443471173164358, parameters k is 9.193949952803832 and b is -35.10608627674131\n",
      "Iteration 939, the loss is 4.443471999251413, parameters k is 9.193857047665492 and b is -35.10616532812471\n",
      "Iteration 940, the loss is 4.44347073447277, parameters k is 9.19400362869316 and b is -35.10620485381641\n",
      "Iteration 941, the loss is 4.443470743780621, parameters k is 9.19391072355482 and b is -35.10628390519981\n",
      "Iteration 942, the loss is 4.443470295781179, parameters k is 9.194057304582488 and b is -35.10632343089151\n",
      "Iteration 943, the loss is 4.443469488309824, parameters k is 9.193964399444148 and b is -35.10640248227491\n",
      "Iteration 944, the loss is 4.4434698570895845, parameters k is 9.194110980471816 and b is -35.106442007966606\n",
      "Iteration 945, the loss is 4.443468369040992, parameters k is 9.194018075333476 and b is -35.10652105935001\n",
      "Iteration 946, the loss is 4.443469282196038, parameters k is 9.193925170195136 and b is -35.10660011073341\n",
      "Iteration 947, the loss is 4.443467930349408, parameters k is 9.194071751222804 and b is -35.10663963642511\n",
      "Iteration 948, the loss is 4.443468026725247, parameters k is 9.193978846084464 and b is -35.10671868780851\n",
      "Iteration 949, the loss is 4.443467491657811, parameters k is 9.194125427112132 and b is -35.10675821350021\n",
      "Iteration 950, the loss is 4.443466771254457, parameters k is 9.194032521973792 and b is -35.10683726488361\n",
      "Iteration 951, the loss is 4.443467052966217, parameters k is 9.19417910300146 and b is -35.106876790575306\n",
      "Iteration 952, the loss is 4.4434655649176245, parameters k is 9.19408619786312 and b is -35.10695584195871\n",
      "Iteration 953, the loss is 4.443466565140666, parameters k is 9.19399329272478 and b is -35.10703489334211\n",
      "Iteration 954, the loss is 4.443465126226038, parameters k is 9.194139873752448 and b is -35.10707441903381\n",
      "Iteration 955, the loss is 4.443465309669872, parameters k is 9.194046968614108 and b is -35.10715347041721\n",
      "Iteration 956, the loss is 4.443464687534449, parameters k is 9.194193549641776 and b is -35.10719299610891\n",
      "Iteration 957, the loss is 4.443464054199073, parameters k is 9.194100644503436 and b is -35.10727204749231\n",
      "Iteration 958, the loss is 4.443464248842855, parameters k is 9.194247225531104 and b is -35.107311573184006\n",
      "Iteration 959, the loss is 4.443462798728285, parameters k is 9.194154320392764 and b is -35.10739062456741\n",
      "Iteration 960, the loss is 4.4434638101512665, parameters k is 9.194300901420432 and b is -35.107430150259106\n",
      "Iteration 961, the loss is 4.4434623221026746, parameters k is 9.194207996282092 and b is -35.10750920164251\n",
      "Iteration 962, the loss is 4.443462592614495, parameters k is 9.194115091143752 and b is -35.10758825302591\n",
      "Iteration 963, the loss is 4.443461883411082, parameters k is 9.19426167217142 and b is -35.10762777871761\n",
      "Iteration 964, the loss is 4.443461337143702, parameters k is 9.19416876703308 and b is -35.10770683010101\n",
      "Iteration 965, the loss is 4.443461444719492, parameters k is 9.194315348060748 and b is -35.107746355792706\n",
      "Iteration 966, the loss is 4.443460081672911, parameters k is 9.194222442922408 and b is -35.10782540717611\n",
      "Iteration 967, the loss is 4.4434610060279, parameters k is 9.194369023950076 and b is -35.107864932867805\n",
      "Iteration 968, the loss is 4.44345951797931, parameters k is 9.194276118811736 and b is -35.10794398425121\n",
      "Iteration 969, the loss is 4.443459875559123, parameters k is 9.194183213673396 and b is -35.10802303563461\n",
      "Iteration 970, the loss is 4.443459079287717, parameters k is 9.194329794701064 and b is -35.10806256132631\n",
      "Iteration 971, the loss is 4.443458620088325, parameters k is 9.194236889562724 and b is -35.10814161270971\n",
      "Iteration 972, the loss is 4.443458640596124, parameters k is 9.194383470590392 and b is -35.108181138401406\n",
      "Iteration 973, the loss is 4.443457364617529, parameters k is 9.194290565452052 and b is -35.10826018978481\n",
      "Iteration 974, the loss is 4.443458201904538, parameters k is 9.19443714647972 and b is -35.108299715476505\n",
      "Iteration 975, the loss is 4.443456713855938, parameters k is 9.19434424134138 and b is -35.10837876685991\n",
      "Iteration 976, the loss is 4.443457158503744, parameters k is 9.19425133620304 and b is -35.10845781824331\n",
      "Iteration 977, the loss is 4.4434562751643485, parameters k is 9.194397917230708 and b is -35.10849734393501\n",
      "Iteration 978, the loss is 4.443455903032956, parameters k is 9.194305012092368 and b is -35.10857639531841\n",
      "Iteration 979, the loss is 4.4434558364727605, parameters k is 9.194451593120036 and b is -35.108615921010106\n",
      "Iteration 980, the loss is 4.443454647562157, parameters k is 9.194358687981696 and b is -35.10869497239351\n",
      "Iteration 981, the loss is 4.443455397781166, parameters k is 9.194505269009364 and b is -35.108734498085205\n",
      "Iteration 982, the loss is 4.443453909732574, parameters k is 9.194412363871024 and b is -35.10881354946861\n",
      "Iteration 983, the loss is 4.443454441448369, parameters k is 9.194319458732684 and b is -35.10889260085201\n",
      "Iteration 984, the loss is 4.4434534710409865, parameters k is 9.194466039760352 and b is -35.108932126543706\n",
      "Iteration 985, the loss is 4.443453185977579, parameters k is 9.194373134622012 and b is -35.10901117792711\n",
      "Iteration 986, the loss is 4.443453032349397, parameters k is 9.19451971564968 and b is -35.109050703618806\n",
      "Iteration 987, the loss is 4.443451930506786, parameters k is 9.19442681051134 and b is -35.10912975500221\n",
      "Iteration 988, the loss is 4.443452593657805, parameters k is 9.194573391539008 and b is -35.109169280693905\n",
      "Iteration 989, the loss is 4.4434511056092045, parameters k is 9.194480486400668 and b is -35.10924833207731\n",
      "Iteration 990, the loss is 4.443451724392996, parameters k is 9.194387581262328 and b is -35.10932738346071\n",
      "Iteration 991, the loss is 4.4434506669176175, parameters k is 9.194534162289996 and b is -35.109366909152406\n",
      "Iteration 992, the loss is 4.443450468922204, parameters k is 9.194441257151656 and b is -35.10944596053581\n",
      "Iteration 993, the loss is 4.443450228226027, parameters k is 9.194587838179324 and b is -35.109485486227506\n",
      "Iteration 994, the loss is 4.443449213451413, parameters k is 9.194494933040984 and b is -35.10956453761091\n",
      "Iteration 995, the loss is 4.44344978953444, parameters k is 9.194641514068651 and b is -35.109604063302605\n",
      "Iteration 996, the loss is 4.4434483014858435, parameters k is 9.194548608930312 and b is -35.10968311468601\n",
      "Iteration 997, the loss is 4.44344900733762, parameters k is 9.194455703791972 and b is -35.10976216606941\n",
      "Iteration 998, the loss is 4.443447862794254, parameters k is 9.19460228481964 and b is -35.109801691761106\n",
      "Iteration 999, the loss is 4.443447751866827, parameters k is 9.1945093796813 and b is -35.10988074314451\n"
     ]
    }
   ],
   "source": [
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "iteration_num = 1000 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = grad_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = grad_b(X_rm,y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a397f4cb70>]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
