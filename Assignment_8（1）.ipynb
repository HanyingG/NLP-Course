{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a kind of neural network that can learns how to compress and encode data, and learn how to construct the data back from the reduced encoded representation to one that close to the original input. It's a combination of encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy search is to get the locally optimal result at each stage and then start searching from the branch of the local optimal result. While beam search is to explore the globally top n result at each stage and take the final optimal result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention is trying to solve the long distance problem in encoder-decoder model by developing the context vector at each stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One word in different context may have different meaning, but in previous word embedding it only have one representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bidirectional language model with multi layer of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it can solve the long distance problem in RNN. Transformer doesn't require a sequence as RNN, it reduces the distance between two words from any position to a constant, so it will improve the parrell computing capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization normalizes the input features across the batch dimension. The key feature of layer normalization is that it normalizes the inputs across the features.In RNN the batch size would be small thus may not well represent the holistic distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because transformer reduces the distance between two words from any position to a constant, thus we need position embedding to indicates the sequence relationship of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention is to set the same input of Q,V,K to undergo attention calculation and get the attention score between this word with other words in the context and get weighted vector. Multi_head attention is to have different representation of Q,K,V and then concate the result together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could use fine tuning of the pretrained GPT to other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it typically mask one or more of words in a sentence and have the model predict those masked words given the other words in sentence.In BERT, 80% oftime replace the word with the [mask] token, 10% of time replace the word with a random word, and 10 % of the time keep the word unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input could be a pair of sentence or a single sentence, where the sentence is a tken sequence. A token is represented by summing the corresponding token, segment and position embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same pre-trained model parameters are used to initialize models for different down-stream tasks,, and fine tune the model based tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT is using multi layer of single direction of transformer, while BERT use a bidirectional multi layer transformer. GPT2 would be the same structure as GPT but with more parameter as well as more and better data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
